{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 基于MindSpore实现二维线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验将实现基于MindSpore的二维线性回归，包括生成数据集、模型构建、模型预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1、实验目的\n",
    "- 掌握如何使用MindSpore实现二维线性回归模型。\n",
    "- 了解如何使用MindSpore的Adam优化器和MSE损失函数。\n",
    "- 了解基于MindSpore的模型训练和模型预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2、整体模型介绍\n",
    "- 二维线性回归原理\n",
    "\n",
    "在机器学习领域，线性回归模型记为：\n",
    "$$y=w_0x_0+w_1x_1+\\cdots+w_nx_n+b=[w_0 w_1 w_2 \\cdots w_n][x_0 x_1 x_2 \\cdots x_n]^T+b$$\n",
    "可以统一形式为：\n",
    "$$y=\\sum_{i=0}^{n}w_ix_i+b=w^Tx+b$$\n",
    "\n",
    "\n",
    "并且我们定义损失函数来度量模型一次预测的好坏，即预测值$\\widehat y$和真实值y的误差，线性损失函数一般取$L=\\frac{1}{2}(y-\\widehat y)^2$，平方损失函数的几何意义是欧氏距离。\n",
    "\n",
    "之后我们便可进行模型训练，采用梯度下降方法求模型参数w，使损失函数最小。\n",
    "\n",
    "梯度下降法顺着当前点梯度反方向，按规定步长$\\alpha$进行迭代搜索，对第i个模型参数进行如下更新：\n",
    "$$w_{i+1}=w_i-\\alpha \\frac{\\partial L(w)}{\\partial (w_i)}$$\n",
    "因为\n",
    "$$\\frac{\\partial L(w)}{\\partial (w_i)}=-\\sum_{j=0}^{m}[y^{(j)}-\\sum_{i=0}^{n}w_i x_i^{(j)}-b]*x_i^{(j)}$$\n",
    "所以\n",
    "$$w_{i+1}=w_i+\\alpha [\\sum_{j=0}^{m}(y^{(j)}-\\sum_{i=0}^{n}w_i x_i^{(j)}-b)*x_i^{(j)}]$$\n",
    "对每个模型参数迭代训练直到收敛即可。因此二维线性回归的自变量为两个，模型便为：\n",
    "$$y=w_0x_0+w_1x_1+b=w^Tx+b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此我们需要定义一个模型Net实现二维线性回归功能。<br>\n",
    "可以调用MindSpore的nn.MSELoss计算预测值和目标值之间的均方误差。<br>\n",
    "调用MindSpore的nn.Adam计算最优参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=2.4；Python环境=3.11。\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU |Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4、数据处理\n",
    "#### 4.1 数据准备\n",
    "为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。任务是使用这个有限样本的数据集来恢复这个模型的参数。我们需要生成一个包含1000个样本的数据集，每个样本包含从标准正态分布中采样的2个特征。 合成的数据集是一个矩阵$X∈R^{(1000×2)}$。我们使用线性模型参数$w =[2,-3.4]^T$、b=3.2和噪声项$\\epsilon$生成数据集及其标签：\n",
    "$y=Xw+b+\\epsilon=w_0x_0+w_1x_1+b+\\epsilon$\n",
    "\n",
    "$\\epsilon$可以视为模型预测和标签之间的观测误差，我们假设ϵ服从均值为0、标准差为0.01的正态分布。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random库实现了各种分布的伪随机数生成器；dtype是MindSpore数据类型的对象；mindspore.ops提供对神经网络层的各种操作；pyplot是常用的绘图模块，能很方便让用户绘制 2D 图表；MindSpore中的Tensor是张量，可放在gpu上加速。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "origin_pos": 2,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import mindspore\n",
    "from mindspore import dtype as mstype\n",
    "import mindspore.ops as ops\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "origin_pos": 7,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "def synthetic_data(w, b, num_examples):  \n",
    "    print((num_examples, len(w)))\n",
    "    # 生成X\n",
    "    X = np.random.normal(0, 1, (num_examples, len(w))).astype(np.float32)\n",
    "    # y = Xw + b\n",
    "    y = np.matmul(X, w) + b \n",
    "    # y = Xw + b + 噪声\n",
    "    y += np.random.normal(0, 0.01, len(y)).astype(np.float32)          \n",
    "    return X, y.reshape((-1, 1))\n",
    "\n",
    "mindspore.set_seed(1)\n",
    "true_w = np.array([2, -3.4]).astype(np.float32)\n",
    "true_b = np.float32(4.2)\n",
    "# 生成数据\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 数据加载\n",
    "数据features中的每一行都包含一个二维数据样本，真实值labels中的每一行都包含一维标签值（一个标量）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "origin_pos": 9,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: [[ 1.6243454  -0.6117564 ]\n",
      " [-0.5281718  -1.0729686 ]\n",
      " [ 0.86540765 -2.3015387 ]\n",
      " [ 1.7448118  -0.7612069 ]] \n",
      "label: [[ 9.533558]\n",
      " [ 6.794138]\n",
      " [13.751566]\n",
      " [10.271619]]\n"
     ]
    }
   ],
   "source": [
    "print('features:', features[0:4],'\\nlabel:', labels[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, labels, train_ratio=0.9):\n",
    "    \"\"\"\n",
    "    将数据集划分为训练集和测试集\n",
    "    :param data: NumPy数组，包含所有数据\n",
    "    :param labels: NumPy数组，包含所有标签\n",
    "    :param train_ratio: 浮点数，训练集的比例\n",
    "    :return: 训练集数据，测试集数据，训练集标签，测试集标签\n",
    "    \"\"\"\n",
    "    assert len(data) == len(labels), \"数据和标签长度不匹配\"\n",
    "    num_samples = len(data)\n",
    "    indices = np.random.permutation(num_samples)\n",
    "    num_train_samples = int(num_samples * train_ratio)\n",
    "    train_indices = indices[:num_train_samples]\n",
    "    test_indices = indices[num_train_samples:]\n",
    "    \n",
    "    X_train = data[train_indices]\n",
    "    y_train = labels[train_indices]\n",
    "    X_test = data[test_indices]\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = split_dataset(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(900, 100, 900, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_features), len(test_features), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用MindSpore的GeneratorDataset创建可迭代数据。<br>\n",
    "dataset模块提供了加载和处理数据集的API；numpy提供了一系列类NumPy接口。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "origin_pos": 16,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225\n"
     ]
    }
   ],
   "source": [
    "from mindspore import dataset as ds  \n",
    "\n",
    "class train_DatasetGenerator:\n",
    "    def __init__(self):\n",
    "        self.data = train_features\n",
    "        self.label = train_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "batch_size = 4\n",
    "dataset_generator = train_DatasetGenerator()\n",
    "train_dataset = ds.GeneratorDataset(dataset_generator, [\"data\", \"label\"], shuffle=True)\n",
    "# 将数据集中连续10条数据合并为一个批处理数据\n",
    "train_dataset = train_dataset.batch(batch_size)  \n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "class test_DatasetGenerator:\n",
    "    def __init__(self):\n",
    "        self.data = test_features\n",
    "        self.label = test_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset_generator = test_DatasetGenerator()\n",
    "test_dataset = ds.GeneratorDataset(dataset_generator, [\"data\", \"label\"], shuffle=True) \n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型构建分为定义实现二维线性回归功能的Net、用于计算预测值与标签值之间的均方误差MSELoss、Adam优化器。\n",
    "- 定义模型Net\n",
    "\n",
    "Net输入为X样本，计算出预测值$\\widehat y$并返回。<br>\n",
    "mindspore.nn用于构建神经网络中的预定义构建块或计算单元；Parameter 是 Tensor 的子类，当它们被绑定为Cell的属性时，会自动添加到其参数列表中，并且可以通过Cell的某些方法获取；mindspore.common.initializer用于初始化神经元参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mindspore.nn as nn\n",
    "from mindspore import Parameter\n",
    "from mindspore.common.initializer import initializer, Zero, Normal\n",
    "\n",
    "\n",
    "def linreg(x, w, b):\n",
    "    # y = Xw+b\n",
    "    return ops.matmul(x, w) + b    \n",
    "\n",
    "\n",
    "class Net(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w = Parameter(initializer(Normal(0.01, 0), (2, 1), mstype.float32))\n",
    "        self.b = Parameter(initializer(Zero(), 1, mstype.float32))\n",
    "        \n",
    "    def construct(self, x):\n",
    "        # y_hat = Xw+b\n",
    "        y_hat = linreg(x, self.w, self.b)  \n",
    "        return y_hat\n",
    "    \n",
    "    \n",
    "# Net用于实现二维线性回归\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用MSE损失函数和Adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练的epoch为3\n",
    "num_epochs = 3\n",
    "# 学习率为0.03\n",
    "lr = 0.03\n",
    "# Adam优化器\n",
    "optimizer = nn.Adam(net.trainable_params(), learning_rate=lr)          \n",
    "# 计算预测值与标签值之间的均方误差\n",
    "loss_fn = nn.MSELoss()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6、模型训练\n",
    "Model是模型训练与推理的高阶接口，调用Model.train方法，传入数据集即可完成模型训练，其中LossMonitor()会监控训练的损失，并将epoch、step、loss信息打印出来，若loss变为NAN或INF，则会终止训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define forward function\n",
    "def forward_fn(data, label):\n",
    "    logits = net(data)\n",
    "    loss = loss_fn(logits, label)\n",
    "    return loss, logits\n",
    "\n",
    "# 2. Get gradient function\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "# 3. Define function of one-step training\n",
    "def train_step(data, label):\n",
    "    (loss, _), grads = grad_fn(data, label)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def train(model, dataset):\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "\n",
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test: Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 14.060421  [  0/225]\n",
      "loss: 21.583872  [ 10/225]\n",
      "loss: 17.625809  [ 20/225]\n",
      "loss: 35.062145  [ 30/225]\n",
      "loss: 5.402611  [ 40/225]\n",
      "loss: 29.139320  [ 50/225]\n",
      "loss: 18.576349  [ 60/225]\n",
      "loss: 7.512076  [ 70/225]\n",
      "loss: 7.636040  [ 80/225]\n",
      "loss: 5.560393  [ 90/225]\n",
      "loss: 2.784903  [100/225]\n",
      "loss: 1.728753  [110/225]\n",
      "loss: 10.996404  [120/225]\n",
      "loss: 7.493189  [130/225]\n",
      "loss: 2.314899  [140/225]\n",
      "loss: 2.995142  [150/225]\n",
      "loss: 0.536584  [160/225]\n",
      "loss: 0.522829  [170/225]\n",
      "loss: 1.050164  [180/225]\n",
      "loss: 0.975079  [190/225]\n",
      "loss: 0.891810  [200/225]\n",
      "loss: 0.564698  [210/225]\n",
      "loss: 0.064260  [220/225]\n",
      "Test: Avg loss: 0.250020 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.081173  [  0/225]\n",
      "loss: 0.130326  [ 10/225]\n",
      "loss: 0.102847  [ 20/225]\n",
      "loss: 0.161158  [ 30/225]\n",
      "loss: 0.034924  [ 40/225]\n",
      "loss: 0.084393  [ 50/225]\n",
      "loss: 0.036734  [ 60/225]\n",
      "loss: 0.011196  [ 70/225]\n",
      "loss: 0.009928  [ 80/225]\n",
      "loss: 0.010508  [ 90/225]\n",
      "loss: 0.003295  [100/225]\n",
      "loss: 0.001847  [110/225]\n",
      "loss: 0.007980  [120/225]\n",
      "loss: 0.004520  [130/225]\n",
      "loss: 0.000728  [140/225]\n",
      "loss: 0.001998  [150/225]\n",
      "loss: 0.000584  [160/225]\n",
      "loss: 0.000312  [170/225]\n",
      "loss: 0.000606  [180/225]\n",
      "loss: 0.000058  [190/225]\n",
      "loss: 0.000432  [200/225]\n",
      "loss: 0.000109  [210/225]\n",
      "loss: 0.000135  [220/225]\n",
      "Test: Avg loss: 0.000117 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.000032  [  0/225]\n",
      "loss: 0.000036  [ 10/225]\n",
      "loss: 0.000088  [ 20/225]\n",
      "loss: 0.000103  [ 30/225]\n",
      "loss: 0.000108  [ 40/225]\n",
      "loss: 0.000073  [ 50/225]\n",
      "loss: 0.000128  [ 60/225]\n",
      "loss: 0.000089  [ 70/225]\n",
      "loss: 0.000110  [ 80/225]\n",
      "loss: 0.000280  [ 90/225]\n",
      "loss: 0.000120  [100/225]\n",
      "loss: 0.000314  [110/225]\n",
      "loss: 0.000030  [120/225]\n",
      "loss: 0.000129  [130/225]\n",
      "loss: 0.000076  [140/225]\n",
      "loss: 0.000330  [150/225]\n",
      "loss: 0.000105  [160/225]\n",
      "loss: 0.000103  [170/225]\n",
      "loss: 0.000016  [180/225]\n",
      "loss: 0.000115  [190/225]\n",
      "loss: 0.000258  [200/225]\n",
      "loss: 0.000018  [210/225]\n",
      "loss: 0.000159  [220/225]\n",
      "Test: Avg loss: 0.000080 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(net, train_dataset)\n",
    "    test(net, test_dataset, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7、模型预测\n",
    "训练3个epoch后输出w和b的估计误差，比较真实参数和通过训练学到的参数来评估训练的成功程度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "origin_pos": 35,
    "tab": [
     "pytorch"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w的估计误差: [Tensor(shape=[], dtype=Float32, value= -3.29018e-05)\n",
      " Tensor(shape=[], dtype=Float32, value= -0.000446081)]\n",
      "b的估计误差: [Tensor(shape=[], dtype=Float32, value= 0.00028801)]\n"
     ]
    }
   ],
   "source": [
    "# w的真实值和训练值之差\n",
    "print(f'w的估计误差: {true_w - net.trainable_params()[0].reshape(true_w.shape)}')  \n",
    "# b的真实值和训练值之差\n",
    "print(f'b的估计误差: {true_b - net.trainable_params()[1]}')                        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
