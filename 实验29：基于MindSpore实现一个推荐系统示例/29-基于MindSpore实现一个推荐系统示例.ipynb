{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da8995b",
   "metadata": {},
   "source": [
    "## 基于MindSpore实现一个推荐系统示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500bd56",
   "metadata": {},
   "source": [
    "在本实验中，我们将基于MindSpore实现一个推荐系统示例，包括Movielens数据集处理、逻辑回归模型构建、模型预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08be58",
   "metadata": {},
   "source": [
    "### 1.实验目的\n",
    "- 掌握Movielens数据集处理。\n",
    "- 掌握基于MindSpore定义逻辑回归模型实现一个推荐系统。\n",
    "- 掌握基于MindSpore读取数据集并进行训练预测CTR(Click-Through-Rate)，即点击率。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0bdf4",
   "metadata": {},
   "source": [
    "### 2.推荐系统原理介绍\n",
    "- 推荐系统\n",
    "\n",
    "推荐系统是一种人工智能或人工智能算法，通常与机器学习相关，使用大数据向消费者建议或推荐其他产品。这些推荐可以基于各种标准，包括过去的购买、搜索历史记录、人口统计信息和其他因素。推荐系统非常有用，因为它们可以帮助用户了解自己无法自行找到的产品和服务。推荐系统可以分为基于内容的推荐、协同过滤推荐、基于知识的推荐以及混合推荐。\n",
    "\n",
    "而逻辑回归是推荐系统最常用的算法之一，相比于协同过滤模型仅利用用户与行为的相互信息进行推荐，逻辑回归能够综合用户、物品、上下文等不同特征，生成较为全面的推荐系统，逻辑回归是独立于协同过滤的推荐模型发展的另一主要方向。相比于系统过滤和矩阵分解，逻辑回归将推荐问题视为一个分类问题——点击率(CTR)预估问题。点击率(CTR)是指对移动广告推广活动展示的点击次数比值，CTR 的计算方法是将移动广告推广活动的点击次数除以展示总数量，再将结果用百分比的形式表示。\n",
    "\n",
    "- 逻辑回归原理\n",
    "\n",
    "逻辑回归实际上是一种分类方法，主要用于输出只有两种的两分类问题，所以利用了Sigmoid函数，函数形式为：\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "对于线性的决策边界的情况，边界形式为：\n",
    "$$\\theta_0+\\theta_1x_1+\\cdots+\\theta_nx_n=\\sum_{i=1}^{n}\\theta_ix_i=\\theta^Tx$$\n",
    "因此构造逻辑回归的预测函数为：\n",
    "$$h_{\\theta}(x)=g(\\theta^Tx)=\\frac{1}{1+e^{-\\theta^Tx}}$$\n",
    "函数$h_\\theta(x)$代表结果取1的概率，因此对于输入x分类结果为类别1和类别0的概率分别为：\n",
    "$$P(y=1|x;\\theta)=h_{\\theta}(x)$$\n",
    "$$P(y=0|x;\\theta)=1-h_{\\theta}(x)$$\n",
    "逻辑回归的损失函数如下：\n",
    "$$J(\\theta)=-\\frac{1}{m}\\sum_{i=1}^{n}(y^{(i)}logh_{\\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)})))$$\n",
    "损失函数是基于最大似然估计推导得到的，推导过程如下：<br>\n",
    "因为\n",
    "$$P(y|x;\\theta)=(h_{\\theta}(x))^y(1-h_{\\theta}(x))^{1-y}$$\n",
    "这是将$P(y=1|x;\\theta)$和$P(y=0|x;\\theta)$整合得到的，然后取似然函数得：\n",
    "$$L(\\theta)=\\prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\\theta)=\\prod_{i=1}^{m}(h_{\\theta}(x^{(i)}))^{y^{(i)}}(1-h_{\\theta}(x^{(i)}))^{1-y^{(i)}}$$\n",
    "取对数可得：\n",
    "$$l(\\theta)=logL(\\theta)=\\sum_{i=1}^{m}(y^{(i)}logh_{\\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)})))$$\n",
    "令\n",
    "$$J(\\theta)=-\\frac{1}{m}l(\\theta)$$\n",
    "如此，就得到了Logistic回归的损失函数，即机器学习中的二值交叉熵。<br>\n",
    "定义逻辑回归模型时可以使用MindSpore的Dense层实现逻辑回归中向量计算部分：\n",
    "$$output=activation(X*kernel+bias)$$\n",
    "激活函数选择Sigmoid即可。<br>\n",
    "因此我们需要定义一个模型Network实现逻辑回归功能，使用MindSpore的Dense层。<br>\n",
    "可以调用MindSpore的nn.BCEWithLogitsLoss计算预测值和目标值之间的二值交叉熵损失。<br>\n",
    "调用MindSpore的nn.Adam计算最优参数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c560c",
   "metadata": {},
   "source": [
    "### 3.实验环境\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=2.4；Python环境=3.11。\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  | 软件环境 | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: | :----: |:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU |Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7d82a0",
   "metadata": {},
   "source": [
    "### 4.数据处理\n",
    "#### 4.1 数据准备\n",
    "示例中用到了开源数据集Movielens，它是一个关于电影评分的数据集，包含多个用户对多部电影的评级数据以及电影和用户的信息，Movielens经常用来做推荐系统、机器学习算法的测试数据集，尤其在推荐系统领域，很多著名论文都是基于这个数据集的。\n",
    "数据集的下载地址为https://grouplens.org/datasets/movielens/100k/<br>\n",
    "实验用到的数据集为：ua.base、ua.test、u.user、u.item、u.occupation，我们先从官网下载相关数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039edd9",
   "metadata": {},
   "source": [
    "os模块提供了非常丰富的方法用来处理文件和目录；urllib.request 模块提供了最基本的构造 HTTP （或其他协议如 FTP）请求的方法，利用它可以模拟浏览器的一个请求发起过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c020d7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://files.grouplens.org/datasets/movielens/ml-100k/ua.base\n",
      "File download completed\n",
      "https://files.grouplens.org/datasets/movielens/ml-100k/ua.test\n",
      "File download completed\n",
      "https://files.grouplens.org/datasets/movielens/ml-100k/u.user\n",
      "File download completed\n",
      "https://files.grouplens.org/datasets/movielens/ml-100k/u.item\n",
      "File download completed\n",
      "https://files.grouplens.org/datasets/movielens/ml-100k/u.occupation\n",
      "File download completed\n"
     ]
    }
   ],
   "source": [
    "import os  \n",
    "import mindspore \n",
    "import urllib.request\n",
    "\n",
    "def download_data(file):\n",
    "    # 文件下载地址\n",
    "    url = 'https://files.grouplens.org/datasets/movielens/ml-100k/' + file\n",
    "    print(url)\n",
    "    # 将文件下载到data文件夹中\n",
    "    urllib.request.urlretrieve(url, 'data/'+file)\n",
    "    print('File download completed')    \n",
    "    \n",
    "if os.path.exists('data') is False:\n",
    "    # data文件夹不存在则创建\n",
    "    os.mkdir('data')\n",
    "# 下载本实验用到的五个数据集\n",
    "download_data('ua.base')\n",
    "download_data('ua.test')\n",
    "download_data('u.user')\n",
    "download_data('u.item')\n",
    "download_data('u.occupation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee9cd35",
   "metadata": {},
   "source": [
    "下载完毕我们看一下数据集的内容。show_data()函数用于展示数据前五行。其中：<br>\n",
    "ua.base和ua.test：训练集和测试集，包括用户id、作品id、评分、时间戳，时间戳是自1970年1月1日UTC以来的unix秒。<br>\n",
    "u.user：用户信息数据，包括用户id、年龄、性别、职业、邮政编码，需要将用户特征进行one-hot编码。<br>\n",
    "u.item：电影信息数据，包括作品id、电影名称、上映日期、视频发行日期、IMDb链接、已被one-hot编码的电影类型(共19种，具体类型可查看数据集的README文件)，每个元素以’|’隔开。<br>\n",
    "u.occupation：用户职业信息，包括21种职业，需要进行one-hot编码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81b454c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ua.base & ua.test:\n",
      "1\t1\t5\t874965758\n",
      "\n",
      "1\t2\t3\t876893171\n",
      "\n",
      "1\t3\t4\t878542960\n",
      "\n",
      "1\t4\t3\t876893119\n",
      "\n",
      "1\t5\t3\t889751712\n",
      "\n",
      "u.user:\n",
      "1|24|M|technician|85711\n",
      "\n",
      "2|53|F|other|94043\n",
      "\n",
      "3|23|M|writer|32067\n",
      "\n",
      "4|24|M|technician|43537\n",
      "\n",
      "5|33|F|other|15213\n",
      "\n",
      "u.item:\n",
      "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
      "\n",
      "2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n",
      "\n",
      "3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n",
      "\n",
      "4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0\n",
      "\n",
      "5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0\n",
      "\n",
      "u.occupation:\n",
      "administrator\n",
      "\n",
      "artist\n",
      "\n",
      "doctor\n",
      "\n",
      "educator\n",
      "\n",
      "engineer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "base_path='data'\n",
    "# 训练集路径\n",
    "train_path = os.path.join(base_path, 'ua.base')                        \n",
    "# 测试集路径\n",
    "test_path = os.path.join(base_path, 'ua.test')                         \n",
    "# 用户信息的文件路径\n",
    "user_path = os.path.join(base_path, 'u.user')\n",
    "# 作品信息的文件路径\n",
    "item_path = os.path.join(base_path, 'u.item')\n",
    "# 用户职业的文件路径\n",
    "occupation_path = os.path.join(base_path, 'u.occupation')\n",
    "# 设置全局种子\n",
    "mindspore.set_seed(1)\n",
    "\n",
    "\n",
    "# 默认编码为UTF-8\n",
    "def show_data(data_path, encoding='UTF-8'):\n",
    "    count = 1\n",
    "    with open(data_path, 'r', encoding=encoding) as f:\n",
    "        for cur_line in f.readlines():\n",
    "            print(cur_line)\n",
    "            if count == 5:\n",
    "                break\n",
    "            count += 1\n",
    "\n",
    "            \n",
    "print('ua.base & ua.test:')\n",
    "show_data(train_path)\n",
    "print('u.user:')\n",
    "show_data(user_path)\n",
    "print('u.item:')\n",
    "show_data(item_path, 'ISO-8859-1')\n",
    "print('u.occupation:')\n",
    "show_data(occupation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ebde5c",
   "metadata": {},
   "source": [
    "数据准备阶段我们需要读取评分数据、电影信息、职业、用户，并将评分数据转换为是否已点击。我们以字典形式读取相关数据，将评分大于3的视为已点击，并定义read_data()函数生成训练集与测试集。<br>\n",
    "定义函数\\__read_rating_data(path)以字典的形式读取评分数据，键为用户id和作品id，值为是否已点击，因此需要将评分数据转换为点击与否，我们将三分以上的作品视为已点击。<br>\n",
    "定义函数\\__read_item_hot()以字典形式返回电影信息，键为作品id，值为作品类型被one-hot编码的向量，u.item中每个元素都以’|’隔开，我们只需要已存在于u.item中的one-hot向量。<br>\n",
    "定义\\__read_occupation_hot()以字典形式返回每个职业，键为职业名，值为one-hot编码的向量。每个职业以换行符隔开。<br>\n",
    "定义\\__read_user_hot()以字典形式返回每个用户信息，键为用户id，值为年龄、性别、职业组成的向量。用户信息的每个元素以’|’隔开。<br>\n",
    "定义read_data()调用上述函数来生成训练集和测试集，数据为年龄、性别、职业、作品类型组成的向量，标签为是否已点击。<br>\n",
    "如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "738cad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy工具包提供了一系列类NumPy接口。\n",
    "import numpy as np\n",
    "\n",
    "def get1or0(r):\n",
    "    # 评分大于3视为用户已点击，否则未点击\n",
    "    return 1.0 if r > 3 else 0.0                                \n",
    "\n",
    "# 以字典的形式返回评分数据\n",
    "def __read_rating_data(path):                                          \n",
    "    dataSet = {}\n",
    "    with open(path, 'r') as f:\n",
    "        # 读取每一行\n",
    "        for line in f.readlines():  \n",
    "            # 读取以制表符隔开的用户id、作品id、评分\n",
    "            d = line.strip().split('\\t')   \n",
    "            # 会将评分转换为是否点击\n",
    "            dataSet[(int(d[0]), int(d[1]))] = [get1or0(int(d[2]))]     \n",
    "    return dataSet\n",
    "\n",
    "\n",
    "# 以字典形式返回电影信息\n",
    "def __read_item_hot():                                                 \n",
    "    items = {}\n",
    "    with open(item_path, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f.readlines():\n",
    "            # 读取以'|'隔开的每个元素\n",
    "            d = line.strip().split('|') \n",
    "            # 字典键为作品id，值为作品类型被one-hot编码的向量\n",
    "            items[int(d[0])] = np.array(d[5:], dtype='float64')        \n",
    "    return items\n",
    "\n",
    "\n",
    "# 以字典形式返回每个职业\n",
    "def __read_occupation_hot():                                           \n",
    "    occupations = {}\n",
    "    with open(occupation_path, 'r') as f:\n",
    "        # 读取以换行符隔开的每个职业\n",
    "        names = f.read().strip().split('\\n')                           \n",
    "    length = len(names)\n",
    "    for i in range(length):  \n",
    "        # 为每个职业都生成一个one-hot向量\n",
    "        l = np.zeros(length, dtype='float64')\n",
    "        l[i] = 1\n",
    "        occupations[names[i]] = l\n",
    "    return occupations\n",
    "\n",
    "\n",
    "# 以字典形式返回每个用户信息\n",
    "def __read_user_hot():                                                  \n",
    "    users = {}\n",
    "    gender_dict = {'M': 1, 'F': 0}\n",
    "    # 读取职业信息\n",
    "    occupation_dict = __read_occupation_hot()                           \n",
    "    with open(user_path, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            # 读取以'|'隔开的用户id、年龄、性别、职业、邮政编码\n",
    "            d = line.strip().split('|')                                 \n",
    "            a = np.array([int(d[1]), gender_dict[d[2]]])\n",
    "            # 字典键为用户id，值为年龄、性别、职业组成的向量\n",
    "            users[int(d[0])] = np.append(a, occupation_dict[d[3]])      \n",
    "    return users\n",
    "\n",
    "\n",
    "def read_dataSet(user_dict, item_dict, path):\n",
    "    X, Y = [], []\n",
    "    # 读取评分数据\n",
    "    ratings = __read_rating_data(path)                                  \n",
    "    for k in ratings:\n",
    "        # X为年龄、性别、职业、作品类型组成的向量\n",
    "        X.append(np.append(user_dict[k[0]], item_dict[k[1]])) \n",
    "        # Y为是否已点击\n",
    "        Y.append(ratings[k])                                            \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    user_dict = __read_user_hot()\n",
    "    item_dict = __read_item_hot()\n",
    "    # 返回训练集\n",
    "    trainX, trainY = read_dataSet(user_dict, item_dict, train_path) \n",
    "    # 返回测试集\n",
    "    testX, testY = read_dataSet(user_dict, item_dict, test_path)        \n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da03643",
   "metadata": {},
   "source": [
    "#### 4.2 数据加载\n",
    "定义create_dataset()函数使用MindSpore的GeneratorDataset创建可迭代数据，通过控制变量train来判断生成训练集还是测试集。batch_size为32。模型训练时可直接使用此函数来加载数据。<br>\n",
    "dataset模块提供了加载和处理数据集的API。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af073d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mindspore import dataset as ds  \n",
    "\n",
    "def get_data(train=True):\n",
    "    # 读取训练集和测试集\n",
    "    trainX, trainY, testX, testY = read_data()                          \n",
    "    train_size = len(trainX)\n",
    "    test_size = len(testX)\n",
    "    if train == True:\n",
    "        for i in range(train_size):\n",
    "            # 训练集标签\n",
    "            y = trainY[i]     \n",
    "            # 训练集数据\n",
    "            x = trainX[i]                                               \n",
    "            yield np.array(x[:]).astype(np.float32), np.array([y[0]]).astype(np.float32)\n",
    "    else:\n",
    "        for i in range(test_size):\n",
    "            # 测试集标签\n",
    "            y = testY[i]          \n",
    "            # 测试集数据\n",
    "            x = testX[i]                                                \n",
    "            yield np.array(x[:]).astype(np.float32), np.array([y[:]]).astype(np.float32)\n",
    "\n",
    "def create_dataset(batch_size=32, repeat_size=1, train=True):\n",
    "    # 使用GeneratorDataset创建可迭代数据\n",
    "    input_data = ds.GeneratorDataset(list(get_data(train)), column_names=['data', 'label'])  \n",
    "    # 将数据集中连续32条数据合并为一个批处理数据\n",
    "    input_data = input_data.batch(batch_size)                \n",
    "    # 重复数据集1次\n",
    "    input_data = input_data.repeat(repeat_size)                          \n",
    "    return input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113a5625",
   "metadata": {},
   "source": [
    "### 5.模型构建\n",
    "模型构建分为定义实现逻辑回归功能的Network、定义二值交叉熵损失函数、定义Momentum优化器。其中输入数据为年龄、性别、职业、作品类型组成的向量，标签为是否点击。\n",
    "- 定义模型Network来实现逻辑回归功能\n",
    "\n",
    "定义模型class Network(nn.Cell)，输入维度为42，为年龄、性别、职业、作品类型组成的向量，输出维度为1，是对用户点击概率的预测。直接调用MindSpore的Dense层，激活函数使用Sigmoid，网络的层数可根据自身情况决定，这里是两层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8aa8c85",
   "metadata": {},
   "source": [
    "mindspore.nn用于构建神经网络中的预定义构建块或计算单元；生成一个服从正态分布的随机数组用于初始化Tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d24d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network<\n",
      "  (fc1): Dense<input_channels=42, output_channels=21, has_bias=True>\n",
      "  (fc2): Dense<input_channels=21, output_channels=1, has_bias=True>\n",
      "  (relu): ReLU<>\n",
      "  (sigmoid): Sigmoid<>\n",
      "  >\n"
     ]
    }
   ],
   "source": [
    "import mindspore.nn as nn                                               \n",
    "from mindspore.common.initializer import Normal \n",
    "class Network(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        # 全连接层的输入维度为42， 输出维度为21\n",
    "        self.fc1 = nn.Dense(42, 21, Normal(0.02), Normal(0.02)) \n",
    "        # 输入维度为21，输出维度为1\n",
    "        self.fc2 = nn.Dense(21, 1, Normal(0.02), Normal(0.02))    \n",
    "        # ReLU函数\n",
    "        self.relu = nn.ReLU()                          \n",
    "        # Sigmoid函数\n",
    "        self.sigmoid = nn.Sigmoid()                                   \n",
    "        \n",
    "    def construct(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x    \n",
    "    \n",
    "net = Network()\n",
    "# 查看网络结构\n",
    "print(net)                                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98228fcf",
   "metadata": {},
   "source": [
    "- 使用二值交叉熵损失函数和Adam优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99936277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch为10\n",
    "epochs = 10      \n",
    "# batch_size为32\n",
    "batch_size = 32        \n",
    "# 学习率为0.01\n",
    "learning_rate = 1e-3                                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e78c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算预测值和真实值之间的二值交叉熵损失。\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")       \n",
    "# Adam优化器\n",
    "optimizer = nn.Adam(net.trainable_params(), learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d9677",
   "metadata": {},
   "source": [
    "### 6.模型训练\n",
    "调用train_model方法，传入数据集即可完成模型训练，并将epoch、step、loss信息打印出来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9871a18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 75, loss is 0.6848706603050232\n",
      "epoch: 1 step: 150, loss is 0.6861016750335693\n",
      "epoch: 1 step: 225, loss is 0.6958327889442444\n",
      "epoch: 1 step: 300, loss is 0.6967008709907532\n",
      "epoch: 1 step: 375, loss is 0.6623865962028503\n",
      "epoch: 1 step: 450, loss is 0.6520721912384033\n",
      "epoch: 1 step: 525, loss is 0.7017002701759338\n",
      "epoch: 1 step: 600, loss is 0.6837337613105774\n",
      "epoch: 1 step: 675, loss is 0.6848462820053101\n",
      "epoch: 1 step: 750, loss is 0.6645023822784424\n",
      "epoch: 1 step: 825, loss is 0.6989676356315613\n",
      "epoch: 1 step: 900, loss is 0.673162579536438\n",
      "epoch: 1 step: 975, loss is 0.6862270832061768\n",
      "epoch: 1 step: 1050, loss is 0.6869756579399109\n",
      "epoch: 1 step: 1125, loss is 0.6861284375190735\n",
      "epoch: 1 step: 1200, loss is 0.7391003370285034\n",
      "epoch: 1 step: 1275, loss is 0.6863336563110352\n",
      "epoch: 1 step: 1350, loss is 0.6592581868171692\n",
      "epoch: 1 step: 1425, loss is 0.710106372833252\n",
      "epoch: 1 step: 1500, loss is 0.6816088557243347\n",
      "epoch: 1 step: 1575, loss is 0.6850377321243286\n",
      "epoch: 1 step: 1650, loss is 0.6782819032669067\n",
      "epoch: 1 step: 1725, loss is 0.6933404803276062\n",
      "epoch: 1 step: 1800, loss is 0.7136686444282532\n",
      "epoch: 1 step: 1875, loss is 0.6923818588256836\n",
      "epoch: 1 step: 1950, loss is 0.663158118724823\n",
      "epoch: 1 step: 2025, loss is 0.6925173997879028\n",
      "epoch: 1 step: 2100, loss is 0.6869352459907532\n",
      "epoch: 1 step: 2175, loss is 0.6930938363075256\n",
      "epoch: 1 step: 2250, loss is 0.6656681895256042\n",
      "epoch: 1 step: 2325, loss is 0.6798027753829956\n",
      "epoch: 1 step: 2400, loss is 0.6576094627380371\n",
      "epoch: 1 step: 2475, loss is 0.7098432779312134\n",
      "epoch: 1 step: 2550, loss is 0.7170860171318054\n",
      "epoch: 1 step: 2625, loss is 0.6787503957748413\n",
      "epoch: 1 step: 2700, loss is 0.7212159037590027\n",
      "epoch: 1 step: 2775, loss is 0.6230419874191284\n",
      "epoch: 2 step: 2850, loss is 0.6533253192901611\n",
      "epoch: 2 step: 2925, loss is 0.6830928325653076\n",
      "epoch: 2 step: 3000, loss is 0.6573837399482727\n",
      "epoch: 2 step: 3075, loss is 0.6524326801300049\n",
      "epoch: 2 step: 3150, loss is 0.6880356669425964\n",
      "epoch: 2 step: 3225, loss is 0.6495082974433899\n",
      "epoch: 2 step: 3300, loss is 0.6510052680969238\n",
      "epoch: 2 step: 3375, loss is 0.6435595750808716\n",
      "epoch: 2 step: 3450, loss is 0.646651029586792\n",
      "epoch: 2 step: 3525, loss is 0.6708387136459351\n",
      "epoch: 2 step: 3600, loss is 0.6697200536727905\n",
      "epoch: 2 step: 3675, loss is 0.6316537261009216\n",
      "epoch: 2 step: 3750, loss is 0.7339590787887573\n",
      "epoch: 2 step: 3825, loss is 0.6923815011978149\n",
      "epoch: 2 step: 3900, loss is 0.6570351123809814\n",
      "epoch: 2 step: 3975, loss is 0.690108597278595\n",
      "epoch: 2 step: 4050, loss is 0.673163115978241\n",
      "epoch: 2 step: 4125, loss is 0.690487265586853\n",
      "epoch: 2 step: 4200, loss is 0.6521062850952148\n",
      "epoch: 2 step: 4275, loss is 0.6606801748275757\n",
      "epoch: 2 step: 4350, loss is 0.6226227283477783\n",
      "epoch: 2 step: 4425, loss is 0.6957352757453918\n",
      "epoch: 2 step: 4500, loss is 0.6871557831764221\n",
      "epoch: 2 step: 4575, loss is 0.6821020245552063\n",
      "epoch: 2 step: 4650, loss is 0.6827332377433777\n",
      "epoch: 2 step: 4725, loss is 0.6598760485649109\n",
      "epoch: 2 step: 4800, loss is 0.6616091728210449\n",
      "epoch: 2 step: 4875, loss is 0.6889334917068481\n",
      "epoch: 2 step: 4950, loss is 0.6796020269393921\n",
      "epoch: 2 step: 5025, loss is 0.6750746369361877\n",
      "epoch: 2 step: 5100, loss is 0.6411659717559814\n",
      "epoch: 2 step: 5175, loss is 0.7087149024009705\n",
      "epoch: 2 step: 5250, loss is 0.7021439075469971\n",
      "epoch: 2 step: 5325, loss is 0.6698426008224487\n",
      "epoch: 2 step: 5400, loss is 0.6626865863800049\n",
      "epoch: 2 step: 5475, loss is 0.6505998373031616\n",
      "epoch: 2 step: 5550, loss is 0.6527519226074219\n",
      "epoch: 2 step: 5625, loss is 0.6864835023880005\n",
      "epoch: 3 step: 5700, loss is 0.6729785799980164\n",
      "epoch: 3 step: 5775, loss is 0.6726025938987732\n",
      "epoch: 3 step: 5850, loss is 0.6304982900619507\n",
      "epoch: 3 step: 5925, loss is 0.6775846481323242\n",
      "epoch: 3 step: 6000, loss is 0.6894121766090393\n",
      "epoch: 3 step: 6075, loss is 0.6375426650047302\n",
      "epoch: 3 step: 6150, loss is 0.6432611346244812\n",
      "epoch: 3 step: 6225, loss is 0.7142832279205322\n",
      "epoch: 3 step: 6300, loss is 0.6753470301628113\n",
      "epoch: 3 step: 6375, loss is 0.6384621262550354\n",
      "epoch: 3 step: 6450, loss is 0.6970770359039307\n",
      "epoch: 3 step: 6525, loss is 0.6734361052513123\n",
      "epoch: 3 step: 6600, loss is 0.6539192199707031\n",
      "epoch: 3 step: 6675, loss is 0.6672463417053223\n",
      "epoch: 3 step: 6750, loss is 0.6601454019546509\n",
      "epoch: 3 step: 6825, loss is 0.6976649761199951\n",
      "epoch: 3 step: 6900, loss is 0.7352020740509033\n",
      "epoch: 3 step: 6975, loss is 0.6809300184249878\n",
      "epoch: 3 step: 7050, loss is 0.6600654125213623\n",
      "epoch: 3 step: 7125, loss is 0.6347722411155701\n",
      "epoch: 3 step: 7200, loss is 0.6567597389221191\n",
      "epoch: 3 step: 7275, loss is 0.6773993372917175\n",
      "epoch: 3 step: 7350, loss is 0.6789889335632324\n",
      "epoch: 3 step: 7425, loss is 0.7361379265785217\n",
      "epoch: 3 step: 7500, loss is 0.6219541430473328\n",
      "epoch: 3 step: 7575, loss is 0.6776038408279419\n",
      "epoch: 3 step: 7650, loss is 0.6117638349533081\n",
      "epoch: 3 step: 7725, loss is 0.6145290732383728\n",
      "epoch: 3 step: 7800, loss is 0.670196533203125\n",
      "epoch: 3 step: 7875, loss is 0.6680499315261841\n",
      "epoch: 3 step: 7950, loss is 0.6873016953468323\n",
      "epoch: 3 step: 8025, loss is 0.6953809857368469\n",
      "epoch: 3 step: 8100, loss is 0.6530544757843018\n",
      "epoch: 3 step: 8175, loss is 0.6751713156700134\n",
      "epoch: 3 step: 8250, loss is 0.683998703956604\n",
      "epoch: 3 step: 8325, loss is 0.692175030708313\n",
      "epoch: 3 step: 8400, loss is 0.7242779731750488\n",
      "epoch: 3 step: 8475, loss is 0.64455646276474\n",
      "epoch: 4 step: 8550, loss is 0.6617447733879089\n",
      "epoch: 4 step: 8625, loss is 0.6941020488739014\n",
      "epoch: 4 step: 8700, loss is 0.6560029983520508\n",
      "epoch: 4 step: 8775, loss is 0.7191914916038513\n",
      "epoch: 4 step: 8850, loss is 0.6308141350746155\n",
      "epoch: 4 step: 8925, loss is 0.6496360301971436\n",
      "epoch: 4 step: 9000, loss is 0.6704996228218079\n",
      "epoch: 4 step: 9075, loss is 0.6396295428276062\n",
      "epoch: 4 step: 9150, loss is 0.7172636985778809\n",
      "epoch: 4 step: 9225, loss is 0.6444576978683472\n",
      "epoch: 4 step: 9300, loss is 0.6685353517532349\n",
      "epoch: 4 step: 9375, loss is 0.6759383082389832\n",
      "epoch: 4 step: 9450, loss is 0.6647716164588928\n",
      "epoch: 4 step: 9525, loss is 0.7060186266899109\n",
      "epoch: 4 step: 9600, loss is 0.7042499780654907\n",
      "epoch: 4 step: 9675, loss is 0.6614131331443787\n",
      "epoch: 4 step: 9750, loss is 0.6678932905197144\n",
      "epoch: 4 step: 9825, loss is 0.692075252532959\n",
      "epoch: 4 step: 9900, loss is 0.7096295952796936\n",
      "epoch: 4 step: 9975, loss is 0.6435902714729309\n",
      "epoch: 4 step: 10050, loss is 0.6743654012680054\n",
      "epoch: 4 step: 10125, loss is 0.6523087620735168\n",
      "epoch: 4 step: 10200, loss is 0.6803332567214966\n",
      "epoch: 4 step: 10275, loss is 0.6946669220924377\n",
      "epoch: 4 step: 10350, loss is 0.6340745687484741\n",
      "epoch: 4 step: 10425, loss is 0.6770297884941101\n",
      "epoch: 4 step: 10500, loss is 0.6469480395317078\n",
      "epoch: 4 step: 10575, loss is 0.6680751442909241\n",
      "epoch: 4 step: 10650, loss is 0.6304389238357544\n",
      "epoch: 4 step: 10725, loss is 0.5647687911987305\n",
      "epoch: 4 step: 10800, loss is 0.6701860427856445\n",
      "epoch: 4 step: 10875, loss is 0.7189107537269592\n",
      "epoch: 4 step: 10950, loss is 0.6815999150276184\n",
      "epoch: 4 step: 11025, loss is 0.6902769804000854\n",
      "epoch: 4 step: 11100, loss is 0.6431819200515747\n",
      "epoch: 4 step: 11175, loss is 0.7135312557220459\n",
      "epoch: 4 step: 11250, loss is 0.6673035621643066\n",
      "epoch: 5 step: 11325, loss is 0.6628763675689697\n",
      "epoch: 5 step: 11400, loss is 0.6138747930526733\n",
      "epoch: 5 step: 11475, loss is 0.6949872970581055\n",
      "epoch: 5 step: 11550, loss is 0.666511595249176\n",
      "epoch: 5 step: 11625, loss is 0.687548041343689\n",
      "epoch: 5 step: 11700, loss is 0.7549266815185547\n",
      "epoch: 5 step: 11775, loss is 0.7025907635688782\n",
      "epoch: 5 step: 11850, loss is 0.6490681171417236\n",
      "epoch: 5 step: 11925, loss is 0.6469738483428955\n",
      "epoch: 5 step: 12000, loss is 0.6347254514694214\n",
      "epoch: 5 step: 12075, loss is 0.7019239664077759\n",
      "epoch: 5 step: 12150, loss is 0.6573562622070312\n",
      "epoch: 5 step: 12225, loss is 0.6679991483688354\n",
      "epoch: 5 step: 12300, loss is 0.6859163045883179\n",
      "epoch: 5 step: 12375, loss is 0.698198139667511\n",
      "epoch: 5 step: 12450, loss is 0.7120341658592224\n",
      "epoch: 5 step: 12525, loss is 0.68968665599823\n",
      "epoch: 5 step: 12600, loss is 0.642924964427948\n",
      "epoch: 5 step: 12675, loss is 0.7061765789985657\n",
      "epoch: 5 step: 12750, loss is 0.6723533868789673\n",
      "epoch: 5 step: 12825, loss is 0.6222401857376099\n",
      "epoch: 5 step: 12900, loss is 0.5942643284797668\n",
      "epoch: 5 step: 12975, loss is 0.6651468276977539\n",
      "epoch: 5 step: 13050, loss is 0.7158322334289551\n",
      "epoch: 5 step: 13125, loss is 0.6931352019309998\n",
      "epoch: 5 step: 13200, loss is 0.6567144393920898\n",
      "epoch: 5 step: 13275, loss is 0.7168286442756653\n",
      "epoch: 5 step: 13350, loss is 0.682380199432373\n",
      "epoch: 5 step: 13425, loss is 0.6976713538169861\n",
      "epoch: 5 step: 13500, loss is 0.6184038519859314\n",
      "epoch: 5 step: 13575, loss is 0.709722101688385\n",
      "epoch: 5 step: 13650, loss is 0.6931243538856506\n",
      "epoch: 5 step: 13725, loss is 0.621910572052002\n",
      "epoch: 5 step: 13800, loss is 0.6588419675827026\n",
      "epoch: 5 step: 13875, loss is 0.6238234043121338\n",
      "epoch: 5 step: 13950, loss is 0.7152834534645081\n",
      "epoch: 5 step: 14025, loss is 0.6623100638389587\n",
      "epoch: 5 step: 14100, loss is 0.7432217597961426\n",
      "epoch: 6 step: 14175, loss is 0.7064374089241028\n",
      "epoch: 6 step: 14250, loss is 0.6685124635696411\n",
      "epoch: 6 step: 14325, loss is 0.6482378244400024\n",
      "epoch: 6 step: 14400, loss is 0.6878443360328674\n",
      "epoch: 6 step: 14475, loss is 0.70474773645401\n",
      "epoch: 6 step: 14550, loss is 0.6338467597961426\n",
      "epoch: 6 step: 14625, loss is 0.6837635040283203\n",
      "epoch: 6 step: 14700, loss is 0.6775454878807068\n",
      "epoch: 6 step: 14775, loss is 0.7497679591178894\n",
      "epoch: 6 step: 14850, loss is 0.6854467988014221\n",
      "epoch: 6 step: 14925, loss is 0.7171513438224792\n",
      "epoch: 6 step: 15000, loss is 0.682664155960083\n",
      "epoch: 6 step: 15075, loss is 0.7117853760719299\n",
      "epoch: 6 step: 15150, loss is 0.7154459357261658\n",
      "epoch: 6 step: 15225, loss is 0.653059184551239\n",
      "epoch: 6 step: 15300, loss is 0.7142664194107056\n",
      "epoch: 6 step: 15375, loss is 0.6870558261871338\n",
      "epoch: 6 step: 15450, loss is 0.6993480920791626\n",
      "epoch: 6 step: 15525, loss is 0.6938089728355408\n",
      "epoch: 6 step: 15600, loss is 0.6381691694259644\n",
      "epoch: 6 step: 15675, loss is 0.6887731552124023\n",
      "epoch: 6 step: 15750, loss is 0.6798752546310425\n",
      "epoch: 6 step: 15825, loss is 0.6796553134918213\n",
      "epoch: 6 step: 15900, loss is 0.7021278142929077\n",
      "epoch: 6 step: 15975, loss is 0.6769151091575623\n",
      "epoch: 6 step: 16050, loss is 0.6779896020889282\n",
      "epoch: 6 step: 16125, loss is 0.7132121920585632\n",
      "epoch: 6 step: 16200, loss is 0.7207663655281067\n",
      "epoch: 6 step: 16275, loss is 0.7593641877174377\n",
      "epoch: 6 step: 16350, loss is 0.626473605632782\n",
      "epoch: 6 step: 16425, loss is 0.6734184622764587\n",
      "epoch: 6 step: 16500, loss is 0.6967419385910034\n",
      "epoch: 6 step: 16575, loss is 0.6861554980278015\n",
      "epoch: 6 step: 16650, loss is 0.6868087649345398\n",
      "epoch: 6 step: 16725, loss is 0.661444902420044\n",
      "epoch: 6 step: 16800, loss is 0.6634316444396973\n",
      "epoch: 6 step: 16875, loss is 0.6448342800140381\n",
      "epoch: 6 step: 16950, loss is 0.6850796937942505\n",
      "epoch: 7 step: 17025, loss is 0.6908395290374756\n",
      "epoch: 7 step: 17100, loss is 0.6894593834877014\n",
      "epoch: 7 step: 17175, loss is 0.7237498164176941\n",
      "epoch: 7 step: 17250, loss is 0.6676936745643616\n",
      "epoch: 7 step: 17325, loss is 0.6596457362174988\n",
      "epoch: 7 step: 17400, loss is 0.7159801721572876\n",
      "epoch: 7 step: 17475, loss is 0.675285279750824\n",
      "epoch: 7 step: 17550, loss is 0.6434569358825684\n",
      "epoch: 7 step: 17625, loss is 0.6786293387413025\n",
      "epoch: 7 step: 17700, loss is 0.7005080580711365\n",
      "epoch: 7 step: 17775, loss is 0.6799457669258118\n",
      "epoch: 7 step: 17850, loss is 0.720740795135498\n",
      "epoch: 7 step: 17925, loss is 0.6670759320259094\n",
      "epoch: 7 step: 18000, loss is 0.6545116901397705\n",
      "epoch: 7 step: 18075, loss is 0.654242217540741\n",
      "epoch: 7 step: 18150, loss is 0.6420255303382874\n",
      "epoch: 7 step: 18225, loss is 0.6446270942687988\n",
      "epoch: 7 step: 18300, loss is 0.6849479675292969\n",
      "epoch: 7 step: 18375, loss is 0.6878674626350403\n",
      "epoch: 7 step: 18450, loss is 0.6996186971664429\n",
      "epoch: 7 step: 18525, loss is 0.7085421681404114\n",
      "epoch: 7 step: 18600, loss is 0.6527531743049622\n",
      "epoch: 7 step: 18675, loss is 0.7136930823326111\n",
      "epoch: 7 step: 18750, loss is 0.6427778005599976\n",
      "epoch: 7 step: 18825, loss is 0.6402485370635986\n",
      "epoch: 7 step: 18900, loss is 0.6673128008842468\n",
      "epoch: 7 step: 18975, loss is 0.6648934483528137\n",
      "epoch: 7 step: 19050, loss is 0.6896577477455139\n",
      "epoch: 7 step: 19125, loss is 0.6762270927429199\n",
      "epoch: 7 step: 19200, loss is 0.6734828948974609\n",
      "epoch: 7 step: 19275, loss is 0.6878509521484375\n",
      "epoch: 7 step: 19350, loss is 0.6230943202972412\n",
      "epoch: 7 step: 19425, loss is 0.6832244396209717\n",
      "epoch: 7 step: 19500, loss is 0.6749606728553772\n",
      "epoch: 7 step: 19575, loss is 0.6759319305419922\n",
      "epoch: 7 step: 19650, loss is 0.6362655758857727\n",
      "epoch: 7 step: 19725, loss is 0.7244494557380676\n",
      "epoch: 7 step: 19800, loss is 0.6533786654472351\n",
      "epoch: 8 step: 19875, loss is 0.6455573439598083\n",
      "epoch: 8 step: 19950, loss is 0.6856026649475098\n",
      "epoch: 8 step: 20025, loss is 0.6953781247138977\n",
      "epoch: 8 step: 20100, loss is 0.6351576447486877\n",
      "epoch: 8 step: 20175, loss is 0.704797089099884\n",
      "epoch: 8 step: 20250, loss is 0.684744119644165\n",
      "epoch: 8 step: 20325, loss is 0.6734144687652588\n",
      "epoch: 8 step: 20400, loss is 0.6350879073143005\n",
      "epoch: 8 step: 20475, loss is 0.6841063499450684\n",
      "epoch: 8 step: 20550, loss is 0.6442856192588806\n",
      "epoch: 8 step: 20625, loss is 0.6625292301177979\n",
      "epoch: 8 step: 20700, loss is 0.6551810503005981\n",
      "epoch: 8 step: 20775, loss is 0.6287720203399658\n",
      "epoch: 8 step: 20850, loss is 0.7131553888320923\n",
      "epoch: 8 step: 20925, loss is 0.6546959280967712\n",
      "epoch: 8 step: 21000, loss is 0.6809348464012146\n",
      "epoch: 8 step: 21075, loss is 0.759485125541687\n",
      "epoch: 8 step: 21150, loss is 0.7379492521286011\n",
      "epoch: 8 step: 21225, loss is 0.6694135665893555\n",
      "epoch: 8 step: 21300, loss is 0.6309718489646912\n",
      "epoch: 8 step: 21375, loss is 0.634466290473938\n",
      "epoch: 8 step: 21450, loss is 0.6696739792823792\n",
      "epoch: 8 step: 21525, loss is 0.7275447249412537\n",
      "epoch: 8 step: 21600, loss is 0.6684219837188721\n",
      "epoch: 8 step: 21675, loss is 0.7089377045631409\n",
      "epoch: 8 step: 21750, loss is 0.6752104163169861\n",
      "epoch: 8 step: 21825, loss is 0.7200941443443298\n",
      "epoch: 8 step: 21900, loss is 0.6391105055809021\n",
      "epoch: 8 step: 21975, loss is 0.6322197318077087\n",
      "epoch: 8 step: 22050, loss is 0.6776905059814453\n",
      "epoch: 8 step: 22125, loss is 0.6728124618530273\n",
      "epoch: 8 step: 22200, loss is 0.66357421875\n",
      "epoch: 8 step: 22275, loss is 0.6790302991867065\n",
      "epoch: 8 step: 22350, loss is 0.6400706171989441\n",
      "epoch: 8 step: 22425, loss is 0.656849205493927\n",
      "epoch: 8 step: 22500, loss is 0.6367571949958801\n",
      "epoch: 8 step: 22575, loss is 0.6187989711761475\n",
      "epoch: 9 step: 22650, loss is 0.6787581443786621\n",
      "epoch: 9 step: 22725, loss is 0.6165028810501099\n",
      "epoch: 9 step: 22800, loss is 0.664672315120697\n",
      "epoch: 9 step: 22875, loss is 0.6435225009918213\n",
      "epoch: 9 step: 22950, loss is 0.6216109395027161\n",
      "epoch: 9 step: 23025, loss is 0.7142161130905151\n",
      "epoch: 9 step: 23100, loss is 0.6632384657859802\n",
      "epoch: 9 step: 23175, loss is 0.6695481538772583\n",
      "epoch: 9 step: 23250, loss is 0.6003648638725281\n",
      "epoch: 9 step: 23325, loss is 0.7738168835639954\n",
      "epoch: 9 step: 23400, loss is 0.7592060565948486\n",
      "epoch: 9 step: 23475, loss is 0.6548122763633728\n",
      "epoch: 9 step: 23550, loss is 0.6516372561454773\n",
      "epoch: 9 step: 23625, loss is 0.6933928728103638\n",
      "epoch: 9 step: 23700, loss is 0.7313051223754883\n",
      "epoch: 9 step: 23775, loss is 0.6809288859367371\n",
      "epoch: 9 step: 23850, loss is 0.682985782623291\n",
      "epoch: 9 step: 23925, loss is 0.6800921559333801\n",
      "epoch: 9 step: 24000, loss is 0.6838963627815247\n",
      "epoch: 9 step: 24075, loss is 0.6553730964660645\n",
      "epoch: 9 step: 24150, loss is 0.6466736197471619\n",
      "epoch: 9 step: 24225, loss is 0.670966386795044\n",
      "epoch: 9 step: 24300, loss is 0.7029815912246704\n",
      "epoch: 9 step: 24375, loss is 0.6776823401451111\n",
      "epoch: 9 step: 24450, loss is 0.6489458680152893\n",
      "epoch: 9 step: 24525, loss is 0.7074363231658936\n",
      "epoch: 9 step: 24600, loss is 0.6429372429847717\n",
      "epoch: 9 step: 24675, loss is 0.6739602088928223\n",
      "epoch: 9 step: 24750, loss is 0.7507968544960022\n",
      "epoch: 9 step: 24825, loss is 0.6574246287345886\n",
      "epoch: 9 step: 24900, loss is 0.6540002822875977\n",
      "epoch: 9 step: 24975, loss is 0.574675977230072\n",
      "epoch: 9 step: 25050, loss is 0.6027435064315796\n",
      "epoch: 9 step: 25125, loss is 0.7107058763504028\n",
      "epoch: 9 step: 25200, loss is 0.6763049364089966\n",
      "epoch: 9 step: 25275, loss is 0.6564109325408936\n",
      "epoch: 9 step: 25350, loss is 0.7237308621406555\n",
      "epoch: 9 step: 25425, loss is 0.6296727657318115\n",
      "epoch: 10 step: 25500, loss is 0.639679491519928\n",
      "epoch: 10 step: 25575, loss is 0.6783207654953003\n",
      "epoch: 10 step: 25650, loss is 0.7014351487159729\n",
      "epoch: 10 step: 25725, loss is 0.6898258328437805\n",
      "epoch: 10 step: 25800, loss is 0.7005888223648071\n",
      "epoch: 10 step: 25875, loss is 0.7385179996490479\n",
      "epoch: 10 step: 25950, loss is 0.7038386464118958\n",
      "epoch: 10 step: 26025, loss is 0.6474184989929199\n",
      "epoch: 10 step: 26100, loss is 0.7141720056533813\n",
      "epoch: 10 step: 26175, loss is 0.6886609792709351\n",
      "epoch: 10 step: 26250, loss is 0.722512423992157\n",
      "epoch: 10 step: 26325, loss is 0.6876554489135742\n",
      "epoch: 10 step: 26400, loss is 0.7193169593811035\n",
      "epoch: 10 step: 26475, loss is 0.6408917307853699\n",
      "epoch: 10 step: 26550, loss is 0.6524398326873779\n",
      "epoch: 10 step: 26625, loss is 0.6434891223907471\n",
      "epoch: 10 step: 26700, loss is 0.6951709985733032\n",
      "epoch: 10 step: 26775, loss is 0.6606916189193726\n",
      "epoch: 10 step: 26850, loss is 0.6905859708786011\n",
      "epoch: 10 step: 26925, loss is 0.6118388175964355\n",
      "epoch: 10 step: 27000, loss is 0.7038534283638\n",
      "epoch: 10 step: 27075, loss is 0.7267391681671143\n",
      "epoch: 10 step: 27150, loss is 0.6348093748092651\n",
      "epoch: 10 step: 27225, loss is 0.5746173858642578\n",
      "epoch: 10 step: 27300, loss is 0.6642407774925232\n",
      "epoch: 10 step: 27375, loss is 0.662984311580658\n",
      "epoch: 10 step: 27450, loss is 0.6572170853614807\n",
      "epoch: 10 step: 27525, loss is 0.7123703956604004\n",
      "epoch: 10 step: 27600, loss is 0.6941062808036804\n",
      "epoch: 10 step: 27675, loss is 0.6950458288192749\n",
      "epoch: 10 step: 27750, loss is 0.6512105464935303\n",
      "epoch: 10 step: 27825, loss is 0.6131430864334106\n",
      "epoch: 10 step: 27900, loss is 0.6552018523216248\n",
      "epoch: 10 step: 27975, loss is 0.7087239027023315\n",
      "epoch: 10 step: 28050, loss is 0.6585213541984558\n",
      "epoch: 10 step: 28125, loss is 0.6227242946624756\n",
      "epoch: 10 step: 28200, loss is 0.7254819273948669\n",
      "epoch: 10 step: 28275, loss is 0.6545352339744568\n"
     ]
    }
   ],
   "source": [
    "from mindspore.train import Model                                       \n",
    "\n",
    "# 创建训练集\n",
    "train_dataset = create_dataset(batch_size)         \n",
    "# 创建测试集\n",
    "test_dataset = create_dataset(batch_size, train=False)                      \n",
    "# 模型训练或推理的高阶接口。Model 会根据用户传入的参数封装可训练或推理的实例\n",
    "model = Model(net, loss_fn=loss_fn, optimizer=optimizer, metrics={\"acc\"})\n",
    "\n",
    "# 定义 forward 函数\n",
    "def forward_fn(inputs, targets):\n",
    "    logits = net(inputs)\n",
    "    loss = loss_fn(logits, targets)\n",
    "    return loss\n",
    "\n",
    "# 调用梯度函数，value_and_grad()为生成求导函数，用于计算给定函数的正向计算结果和梯度\n",
    "grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters)\n",
    "\n",
    "# 定义一步训练的函数\n",
    "def train_step(inputs, targets):\n",
    "    loss, grads = grad_fn(inputs, targets)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "# 训练\n",
    "def train_model(train_dataset, epochs, print_freq):\n",
    "    # 初始化训练过程\n",
    "    global_step = 0  # 记录全局步数\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i, batch_data in enumerate(train_dataset.create_dict_iterator()):\n",
    "            # 执行单步训练\n",
    "            inputs, targets = batch_data[\"data\"], batch_data[\"label\"]\n",
    "            loss = train_step(inputs, targets)\n",
    "            \n",
    "            # 每print_freq步输出一次训练信息\n",
    "            if (global_step + 1) % print_freq == 0:\n",
    "                print(f\"epoch: {epoch + 1} step: {global_step + 1}, loss is {loss.asnumpy()}\")\n",
    "            global_step += 1  # 更新全局步数\n",
    "\n",
    "train_model(train_dataset, epochs, 75)\n",
    "# 将网络权重保存到checkpoint文件中\n",
    "mindspore.save_checkpoint(net, \"./MyNet.ckpt\")                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069df18e",
   "metadata": {},
   "source": [
    "### 7.模型预测\n",
    "使用Model.predict()方法完成测试，计算预测的CTR，与真实的CTR进行对比。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95038807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the predicted CTR is 0.3125\n",
      "the real CTR is 0.6875\n",
      "the predicted CTR is 0.46875\n",
      "the real CTR is 0.6875\n",
      "the predicted CTR is 0.375\n",
      "the real CTR is 0.59375\n",
      "the predicted CTR is 0.46875\n",
      "the real CTR is 0.59375\n",
      "the predicted CTR is 0.3125\n",
      "the real CTR is 0.59375\n",
      "the predicted CTR is 0.40625\n",
      "the real CTR is 0.65625\n",
      "the predicted CTR is 0.4375\n",
      "the real CTR is 0.6875\n",
      "the predicted CTR is 0.25\n",
      "the real CTR is 0.6875\n",
      "the predicted CTR is 0.28125\n",
      "the real CTR is 0.59375\n",
      "The average predicted CTR is 0.3797669491525424\n",
      "The average real CTR is 0.579343220338983\n"
     ]
    }
   ],
   "source": [
    "count = 1\n",
    "pre_ctr_list = []\n",
    "real_ctr_list = []\n",
    "for data, label in test_dataset:\n",
    "    # 模型预测\n",
    "    pre = model.predict(data)                                            \n",
    "    \n",
    "    # 若预测点击概率大于等于0.5，则表示为点击，点击计数加一\n",
    "    click_count = 0\n",
    "    for i in range(pre.shape[0]):\n",
    "        if pre[i] >=0.5:\n",
    "            click_count += 1\n",
    "    # 输出前十个batch的CTR预测结果\n",
    "    if count < 10:\n",
    "        print(\"the predicted CTR is {}\".format(click_count/batch_size))\n",
    "    pre_ctr_list.append(click_count/batch_size)\n",
    "    \n",
    "    # 点击计数的真实值统计\n",
    "    click_count = 0\n",
    "    for i in range(label.shape[0]):\n",
    "        if label[i][0] == 1:\n",
    "            click_count += 1\n",
    "    # 输出前十个batch的CTR真实值\n",
    "    if count < 10:\n",
    "        print(\"the real CTR is {}\".format(click_count/batch_size))\n",
    "    real_ctr_list.append(click_count/batch_size)\n",
    "    \n",
    "    count += 1\n",
    "\n",
    "# 平均CTR预测值\n",
    "print('The average predicted CTR is {}'.format(np.mean(pre_ctr_list)))\n",
    "# 平均CTR真实值\n",
    "print('The average real CTR is {}'.format(np.mean(real_ctr_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c75cc1-85dc-4c4c-b2bb-3b7d0a1d4ca2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
