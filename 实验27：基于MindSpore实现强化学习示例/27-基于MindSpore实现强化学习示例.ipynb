{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5732842c",
   "metadata": {},
   "source": [
    "# 基于MindSpore实现强化学习示例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46d07a",
   "metadata": {},
   "source": [
    "本实验主要介绍强化学习的相关概念，使用Mindspore实现强化学习示例，以DQN算法为例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3344971",
   "metadata": {},
   "source": [
    "## 1、实验目的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e9b235",
   "metadata": {},
   "source": [
    "- 掌握强化学习（Reinforcement Learning）的相关概念。\n",
    "- 掌握如何使用Mindspore实现强化学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f100fb1",
   "metadata": {},
   "source": [
    "## 2、强化学习原理介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70baac5",
   "metadata": {},
   "source": [
    "（1）强化学习（Reinforcement Learning）是机器学习的一个分支，旨在让智能体（agent）通过与环境的交互来学习最优行为策略，以最大化累积奖励或获得特定目标。\n",
    "\n",
    "（2）强化学习的基本元素包括：\n",
    "\n",
    "- 环境（Environment）：智能体与其交互的外部环境，可以是真实世界或模拟环境。\n",
    "- 状态（State）：环境的某个特定时刻的观测值，用于描述环境的特征。\n",
    "- 动作（Action）：智能体在某个状态下可以执行的操作或决策。\n",
    "- 奖励（Reward）：在特定状态下，环境根据智能体的动作给予的反馈信号。\n",
    "- 策略（Policy）：智能体的行为策略，决定在给定状态下采取哪个动作。\n",
    "- 值函数（Value Function）：评估状态或状态-动作对的优劣程度，用于指导决策。\n",
    "- 学习算法（Learning Algorithm）：根据智能体与环境的交互经验，更新策略或值函数的算法。\n",
    "\n",
    "（3）强化学习的目标：\n",
    "\n",
    "通过学习最优策略来最大化累积奖励。为了达到这个目标，智能体通过与环境的交互，观察当前状态，选择动作，接收奖励，并根据奖励信号来调整策略或值函数。强化学习算法可以分为基于值函数的方法（如Q-learning、DQN）和基于策略的方法（如Policy Gradient、Actor-Critic）等。\n",
    "\n",
    "（4）应用\n",
    "\n",
    "强化学习在许多领域都有应用，例如机器人控制、游戏玩法、自动驾驶、金融交易等。它的独特之处在于智能体通过与环境的交互进行学习，而无需依赖标注的数据集，因此适用于很多现实世界的场景，可以在复杂、未知和动态的环境中进行决策和学习\n",
    "\n",
    "（5）DQN 算法\n",
    "\n",
    "DQN（Deep Q-Network）是一种用于解决强化学习问题的深度学习算法。它结合了Q-learning和神经网络，能够学习并估计状态-动作对的Q值函数，通过使用神经网络来逼近Q值函数，实现了对高维状态空间的学习和泛化能力。 \n",
    " \n",
    "DQN算法的基本步骤：\n",
    "   \n",
    "a.初始化Q网络，目标Q网络和经验回放缓冲区。\n",
    "\n",
    "b.对于每个回合（episode）循环：\n",
    "\n",
    "- 重置环境并获取初始状态。\n",
    "- 对于每一步（step）循环：根据当前状态从Q网络中选择一个动作（通常使用ε-greedy策略，其中ε是一个随时间递减的参数）。\n",
    "- 执行选定的动作，观察下一个状态和奖励。\n",
    "- 将转移（当前状态，动作，下一个状态，奖励）存储在经验回放缓冲区中。\n",
    "- 从经验回放缓冲区中随机采样一批转移。\n",
    "- 计算目标Q值：对于每个样本转移，计算目标Q值作为\n",
    "- target_Q = reward + discount_factor * max(Q(next_state, next_action))\n",
    "- 其中 next_action 是从目标Q网络中选择的下一个动作。\n",
    "- 通过最小化目标Q值和当前Q值的均方误差来更新Q网络的参数。\n",
    "- 定期更新目标Q网络的参数，例如每隔一定的步数。\n",
    "- 更新当前状态为下一个状态。\n",
    "- 如果达到终止条件（例如，达到最大步数或解决了环境），则跳出循环。\n",
    "     \n",
    "c.返回训练好的Q网络。\n",
    "   \n",
    "   \n",
    "（6）gym平台介绍\n",
    "\n",
    "本实验借助了gym平台的环境，该平台由OpenAI公司开发，且提供了一整套与平台中虚拟环境进行交互的API接口，gym 的推出为强化学习算法的研究提供了更好地基准测试平台，同时将各类 环境标准化，使研究员可以专注于算法研究而无需花过多的时间在环境的模拟上。gym 提供一个 step 函数供智能体与环境进行交互，其参数为动作，主要返回值及含义分别为：\n",
    "- state：表示智能体所处环境的当前状态，代表着智能体的观察值即状态。\n",
    "- reward：表示智能体采取操作后从环境中获得的奖励，其类型可能是整数、小数等，但是具体的规模和类型与具体的规模和类型与环境有关，但是智能体的总目标仍然是获取最大的奖励值。\n",
    "- done: 大多数任务都属于阶段性任务，当到达一定条件的时候表示任务已经结束，比如五子棋游戏中的一方五子相连，机器人在路面上摔倒，或者在规定的步数以内没有完成任务，则都代表任务结束。所以 done 是一个判断条件，类型为布尔值，代表当前任务是否结束，如果结束则可以选择使用 reset 函数重置当前任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ea34a",
   "metadata": {},
   "source": [
    "## 3、 实验环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c108f839",
   "metadata": {},
   "source": [
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=MindSpore 2.4；Python环境=3.11\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  |          软件环境           | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: |:-----------------------:|:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU |Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d73fca3",
   "metadata": {},
   "source": [
    "安装对应强化学习环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff23aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gym==0.22.0\n",
    "pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d1d2b",
   "metadata": {},
   "source": [
    "## 4、数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38991a1",
   "metadata": {},
   "source": [
    "本实验采用Open Gym中的CartPole-v1环境，DQN的实现主要参考了论文：[Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa378e",
   "metadata": {},
   "source": [
    "### 4.1 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6fa414",
   "metadata": {},
   "source": [
    "在本实验中，需要设置一些超参数，如学习率设为0.01，贪婪度设为0.9，奖励的折扣因子设为0.9等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a841811",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:11.763892Z",
     "start_time": "2025-01-17T16:48:11.758615Z"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper Parameters\n",
    "BATCH_SIZE = 8\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.9               # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "MEMORY_CAPACITY = 2900\n",
    "\n",
    "# EPS_START 是 epsilon 的起始值\n",
    "# EPS_END 是 epsilon 的最终值\n",
    "# TAU is the update rate of the target network\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7730d48",
   "metadata": {},
   "source": [
    "### 4.2 数据加载"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb24c5",
   "metadata": {},
   "source": [
    "加载车杆模型、获取动作数和状态数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cd8c4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:12.018510Z",
     "start_time": "2025-01-17T16:48:11.777906Z"
    },
    "collapsed": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 科学计算库\n",
    "import gym\n",
    "\n",
    "# 加载车杆模型\n",
    "env = gym.make('CartPole-v1')\n",
    "env = env.unwrapped\n",
    "# 获取动作数\n",
    "N_ACTIONS = env.action_space.n\n",
    "# 获取状态数\n",
    "N_STATES = env.observation_space.shape[0]\n",
    "ENV_A_SHAPE = 0 if isinstance(env.action_space.sample(), int) else env.action_space.sample().shape     # to confirm the shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d379b9e0",
   "metadata": {},
   "source": [
    "## 5、模型构建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0197c3",
   "metadata": {},
   "source": [
    "### 5.1 导入Python库和模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac47b50a",
   "metadata": {},
   "source": [
    "在使用前，导入需要的Python库和模块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09ffbc38",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:13.162041Z",
     "start_time": "2025-01-17T16:48:12.071880Z"
    }
   },
   "outputs": [],
   "source": [
    "# MindSpore库\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "# 常见算子操作\n",
    "from mindspore import ops\n",
    "# 引入张量模块\n",
    "from mindspore import Tensor\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b9278",
   "metadata": {},
   "source": [
    "### 5.2 定义神经网络模型 Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47ab83",
   "metadata": {},
   "source": [
    "使用Mindspore框架的nn模块，定义一个神经网络模型 Net。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff33c2c6d82b5c",
   "metadata": {},
   "source": [
    "### 5.3增加重设内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "798a3a40d72325e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:13.185429Z",
     "start_time": "2025-01-17T16:48:13.181062Z"
    }
   },
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e077d2ccd25a5d",
   "metadata": {},
   "source": [
    "构建通用网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cc42fce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:13.215802Z",
     "start_time": "2025-01-17T16:48:13.210850Z"
    }
   },
   "outputs": [],
   "source": [
    "class Net(nn.Cell):\n",
    "    def __init__(self, ):\n",
    "        super(Net, self).__init__()\n",
    "        # 一个全连接层\n",
    "        self.fc1 = nn.Dense(N_STATES, 20)\n",
    "        # 第二个全连接层\n",
    "        self.fc2 = nn.Dense(20, 50)\n",
    "        # 第s三个全连接层\n",
    "        self.fc3 = nn.Dense(50, N_ACTIONS)\n",
    "        # 激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 全连接层\n",
    "        x = self.fc1(x)\n",
    "        # 全连接层\n",
    "        x = self.fc2(x)\n",
    "        # 激活层\n",
    "        x = self.relu(x)\n",
    "        # 全连接层\n",
    "        x = self.fc3(x)\n",
    "        # 输出\n",
    "        actions_value = self.relu(x)\n",
    "        # 返回输出值\n",
    "        return actions_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e4ee5",
   "metadata": {},
   "source": [
    "### 5.3 构建DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf12846",
   "metadata": {},
   "source": [
    "实现DQN（Deep Q-Network）算法的类 DQN，包括网络的初始化、动作选择、记忆存储和学习过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b8bb81",
   "metadata": {},
   "source": [
    "#### 5.3.1 初始化（__init__）：\n",
    "\n",
    "- 初始化DQN智能体。创建两个Net类的实例（eval_net和target_net）。\n",
    "- learn_step_counter用于跟踪目标网络更新的步数。\n",
    "- memory_counter用于跟踪存储在内存中的转换数。\n",
    "- memory数组用于存储转换（状态、动作、奖励、下一个状态）。\n",
    "- optimizer是Adam优化器的实例，用于更新神经网络参数。\n",
    "- loss_func是均方误差（MSE）损失函数，用于计算损失。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "249acffb67458931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:13.272313Z",
     "start_time": "2025-01-17T16:48:13.237490Z"
    }
   },
   "outputs": [],
   "source": [
    "n_actions = env.action_space.n\n",
    "eval_net = Net()\n",
    "target_net = Net()\n",
    "target_net.update_parameters_name('target.')\n",
    "target_net_state_dict = target_net.trainable_params()\n",
    "eval_net_state_dict = eval_net.trainable_params()\n",
    "for t_param, p_param in zip(target_net_state_dict, eval_net_state_dict):\n",
    "    ops.assign(t_param, p_param)\n",
    "\n",
    "criterion = nn.SmoothL1Loss(reduction='mean')\n",
    "optimizer = nn.AdamWeightDecay(eval_net.trainable_params(), learning_rate=LR, eps=1e-08, weight_decay=0.01)\n",
    "memory = ReplayMemory(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa69591",
   "metadata": {},
   "source": [
    "#### 5.3.2 选择动作（choose_action）：\n",
    "- choose_action方法以状态x作为输入，并根据ε-贪心策略选择动作。\n",
    "- 如果随机生成的数小于探索率（EPSILON），则从评估网络（eval_net）中选择具有最高Q值的动作。\n",
    "- 否则，选择一个随机动作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8966944cc15c67bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:13.295442Z",
     "start_time": "2025-01-17T16:48:13.290754Z"
    }
   },
   "outputs": [],
   "source": [
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        # t.max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was\n",
    "        # found, so we pick action with the larger expected reward.\n",
    "        return eval_net(Tensor(state)).argmax(1).view(1, 1).asnumpy()\n",
    "    else:\n",
    "        return np.array([[env.action_space.sample()]], dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89ce4ec",
   "metadata": {},
   "source": [
    "#### 5.3.3 学习过程：\n",
    "\n",
    "首先对一个批次进行采样，将所有张量连接成一个张量，计算$Q(s_t, a_t)$\n",
    " 和$V(s_{t+1}) = \\\\max_a Q(s_{t+1}, a)$ ，并将它们合并到我们的损失中。根据定义，我们设置$V(s) = 0$ 如果 $s$为终止状态。我们还使用目标网络来计算$V(s_{t+1})$ ,增加稳定性。目标网络在每一步都会使用 soft update_ 由之前定义的超参数 TAU 控制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64472dbb90c55298",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:13.347195Z",
     "start_time": "2025-01-17T16:48:13.340095Z"
    }
   },
   "outputs": [],
   "source": [
    "def forward(state_batch, action_batch, reward_batch, non_final_mask, non_final_next_states):\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values =eval_net(state_batch).gather_elements(1, action_batch)\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = non_final_mask * target_net(non_final_next_states).max(1)\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = criterion(state_action_values, expected_state_action_values.expand_dims(1))\n",
    "    return loss\n",
    "\n",
    "grad_fn = ops.value_and_grad(forward, None, optimizer.parameters)\n",
    "\n",
    "def train_step(state_batch, action_batch, reward_batch, non_final_mask, non_final_next_states):\n",
    "    # Optimize the model\n",
    "    loss, grads = grad_fn(state_batch, action_batch, reward_batch, non_final_mask, non_final_next_states)\n",
    "    grads = ops.clip_by_value(grads, -100, 100)\n",
    "    optimizer(grads)\n",
    "    return loss\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    # non_final_mask = mindspore.Tensor(tuple(map(lambda s: s is not None,\n",
    "    #                                       batch.next_state)), dtype=mindspore.bool_)\n",
    "    # non_final_next_states = ops.concat([s if s is not None else ops.zeros((1, 4)) for s in batch.next_state])\n",
    "    # state_batch = ops.concat(batch.state)\n",
    "    # action_batch = ops.concat(batch.action)\n",
    "    # reward_batch = ops.concat(batch.reward)\n",
    "    non_final_mask = Tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=ms.bool_)\n",
    "    non_final_next_states = Tensor(np.concatenate([s if s is not None else np.zeros((1, 4)).astype(np.float32) for s in batch.next_state]))\n",
    "    state_batch = Tensor(np.concatenate(batch.state))\n",
    "    action_batch = Tensor(np.concatenate(batch.action))\n",
    "    reward_batch = Tensor(np.concatenate(batch.reward))\n",
    "    train_step(state_batch, action_batch, reward_batch, non_final_mask, non_final_next_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20f49d",
   "metadata": {},
   "source": [
    "## 6、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cc46f1d991973",
   "metadata": {},
   "source": [
    "模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fbcac8343e468a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T16:48:37.346269Z",
     "start_time": "2025-01-17T16:48:13.370170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Epoch_reward:  30.0\n",
      "Epoch:  1 | Epoch_reward:  31.0\n",
      "Epoch:  2 | Epoch_reward:  21.0\n",
      "Epoch:  3 | Epoch_reward:  21.0\n",
      "Epoch:  4 | Epoch_reward:  30.0\n",
      "Epoch:  5 | Epoch_reward:  19.0\n",
      "Epoch:  6 | Epoch_reward:  12.0\n",
      "Epoch:  7 | Epoch_reward:  21.0\n",
      "Epoch:  8 | Epoch_reward:  11.0\n",
      "Epoch:  9 | Epoch_reward:  19.0\n",
      "Epoch:  10 | Epoch_reward:  13.0\n",
      "Epoch:  11 | Epoch_reward:  16.0\n",
      "Epoch:  12 | Epoch_reward:  14.0\n",
      "Epoch:  13 | Epoch_reward:  24.0\n",
      "Epoch:  14 | Epoch_reward:  11.0\n",
      "Epoch:  15 | Epoch_reward:  14.0\n",
      "Epoch:  16 | Epoch_reward:  13.0\n",
      "Epoch:  17 | Epoch_reward:  17.0\n",
      "Epoch:  18 | Epoch_reward:  10.0\n",
      "Epoch:  19 | Epoch_reward:  29.0\n",
      "Epoch:  20 | Epoch_reward:  21.0\n",
      "Epoch:  21 | Epoch_reward:  17.0\n",
      "Epoch:  22 | Epoch_reward:  31.0\n",
      "Epoch:  23 | Epoch_reward:  13.0\n",
      "Epoch:  24 | Epoch_reward:  30.0\n",
      "Epoch:  25 | Epoch_reward:  11.0\n",
      "Epoch:  26 | Epoch_reward:  14.0\n",
      "Epoch:  27 | Epoch_reward:  24.0\n",
      "Epoch:  28 | Epoch_reward:  17.0\n",
      "Epoch:  29 | Epoch_reward:  13.0\n",
      "Epoch:  30 | Epoch_reward:  16.0\n",
      "Epoch:  31 | Epoch_reward:  11.0\n",
      "Epoch:  32 | Epoch_reward:  8.0\n",
      "Epoch:  33 | Epoch_reward:  17.0\n",
      "Epoch:  34 | Epoch_reward:  38.0\n",
      "Epoch:  35 | Epoch_reward:  16.0\n",
      "Epoch:  36 | Epoch_reward:  25.0\n",
      "Epoch:  37 | Epoch_reward:  21.0\n",
      "Epoch:  38 | Epoch_reward:  16.0\n",
      "Epoch:  39 | Epoch_reward:  17.0\n",
      "Epoch:  40 | Epoch_reward:  26.0\n",
      "Epoch:  41 | Epoch_reward:  10.0\n",
      "Epoch:  42 | Epoch_reward:  10.0\n",
      "Epoch:  43 | Epoch_reward:  14.0\n",
      "Epoch:  44 | Epoch_reward:  15.0\n",
      "Epoch:  45 | Epoch_reward:  11.0\n",
      "Epoch:  46 | Epoch_reward:  34.0\n",
      "Epoch:  47 | Epoch_reward:  16.0\n",
      "Epoch:  48 | Epoch_reward:  11.0\n",
      "Epoch:  49 | Epoch_reward:  20.0\n",
      "Epoch:  50 | Epoch_reward:  13.0\n",
      "Epoch:  51 | Epoch_reward:  10.0\n",
      "Epoch:  52 | Epoch_reward:  11.0\n",
      "Epoch:  53 | Epoch_reward:  9.0\n",
      "Epoch:  54 | Epoch_reward:  9.0\n",
      "Epoch:  55 | Epoch_reward:  15.0\n",
      "Epoch:  56 | Epoch_reward:  10.0\n",
      "Epoch:  57 | Epoch_reward:  13.0\n",
      "Epoch:  58 | Epoch_reward:  9.0\n",
      "Epoch:  59 | Epoch_reward:  11.0\n",
      "Epoch:  60 | Epoch_reward:  25.0\n",
      "Epoch:  61 | Epoch_reward:  10.0\n",
      "Epoch:  62 | Epoch_reward:  9.0\n",
      "Epoch:  63 | Epoch_reward:  11.0\n",
      "Epoch:  64 | Epoch_reward:  30.0\n",
      "Epoch:  65 | Epoch_reward:  12.0\n",
      "Epoch:  66 | Epoch_reward:  12.0\n",
      "Epoch:  67 | Epoch_reward:  16.0\n",
      "Epoch:  68 | Epoch_reward:  20.0\n",
      "Epoch:  69 | Epoch_reward:  19.0\n",
      "Epoch:  70 | Epoch_reward:  16.0\n",
      "Epoch:  71 | Epoch_reward:  9.0\n",
      "Epoch:  72 | Epoch_reward:  10.0\n",
      "Epoch:  73 | Epoch_reward:  14.0\n",
      "Epoch:  74 | Epoch_reward:  30.0\n",
      "Epoch:  75 | Epoch_reward:  25.0\n",
      "Epoch:  76 | Epoch_reward:  22.0\n",
      "Epoch:  77 | Epoch_reward:  31.0\n",
      "Epoch:  78 | Epoch_reward:  8.0\n",
      "Epoch:  79 | Epoch_reward:  11.0\n",
      "Epoch:  80 | Epoch_reward:  11.0\n",
      "Epoch:  81 | Epoch_reward:  12.0\n",
      "Epoch:  82 | Epoch_reward:  10.0\n",
      "Epoch:  83 | Epoch_reward:  26.0\n",
      "Epoch:  84 | Epoch_reward:  25.0\n",
      "Epoch:  85 | Epoch_reward:  9.0\n",
      "Epoch:  86 | Epoch_reward:  20.0\n",
      "Epoch:  87 | Epoch_reward:  11.0\n",
      "Epoch:  88 | Epoch_reward:  13.0\n",
      "Epoch:  89 | Epoch_reward:  12.0\n",
      "Epoch:  90 | Epoch_reward:  40.0\n",
      "Epoch:  91 | Epoch_reward:  10.0\n",
      "Epoch:  92 | Epoch_reward:  45.0\n",
      "Epoch:  93 | Epoch_reward:  11.0\n",
      "Epoch:  94 | Epoch_reward:  18.0\n",
      "Epoch:  95 | Epoch_reward:  17.0\n",
      "Epoch:  96 | Epoch_reward:  11.0\n",
      "Epoch:  97 | Epoch_reward:  10.0\n",
      "Epoch:  98 | Epoch_reward:  11.0\n",
      "Epoch:  99 | Epoch_reward:  82.0\n",
      "Epoch:  100 | Epoch_reward:  12.0\n",
      "Epoch:  101 | Epoch_reward:  13.0\n",
      "Epoch:  102 | Epoch_reward:  30.0\n",
      "Epoch:  103 | Epoch_reward:  12.0\n",
      "Epoch:  104 | Epoch_reward:  16.0\n",
      "Epoch:  105 | Epoch_reward:  9.0\n",
      "Epoch:  106 | Epoch_reward:  8.0\n",
      "Epoch:  107 | Epoch_reward:  11.0\n",
      "Epoch:  108 | Epoch_reward:  28.0\n",
      "Epoch:  109 | Epoch_reward:  9.0\n",
      "Epoch:  110 | Epoch_reward:  18.0\n",
      "Epoch:  111 | Epoch_reward:  9.0\n",
      "Epoch:  112 | Epoch_reward:  17.0\n",
      "Epoch:  113 | Epoch_reward:  13.0\n",
      "Epoch:  114 | Epoch_reward:  18.0\n",
      "Epoch:  115 | Epoch_reward:  13.0\n",
      "Epoch:  116 | Epoch_reward:  12.0\n",
      "Epoch:  117 | Epoch_reward:  9.0\n",
      "Epoch:  118 | Epoch_reward:  10.0\n",
      "Epoch:  119 | Epoch_reward:  11.0\n",
      "Epoch:  120 | Epoch_reward:  10.0\n",
      "Epoch:  121 | Epoch_reward:  14.0\n",
      "Epoch:  122 | Epoch_reward:  9.0\n",
      "Epoch:  123 | Epoch_reward:  11.0\n",
      "Epoch:  124 | Epoch_reward:  11.0\n",
      "Epoch:  125 | Epoch_reward:  9.0\n",
      "Epoch:  126 | Epoch_reward:  10.0\n",
      "Epoch:  127 | Epoch_reward:  8.0\n",
      "Epoch:  128 | Epoch_reward:  9.0\n",
      "Epoch:  129 | Epoch_reward:  10.0\n",
      "Epoch:  130 | Epoch_reward:  9.0\n",
      "Epoch:  131 | Epoch_reward:  9.0\n",
      "Epoch:  132 | Epoch_reward:  10.0\n",
      "Epoch:  133 | Epoch_reward:  11.0\n",
      "Epoch:  134 | Epoch_reward:  20.0\n",
      "Epoch:  135 | Epoch_reward:  11.0\n",
      "Epoch:  136 | Epoch_reward:  11.0\n",
      "Epoch:  137 | Epoch_reward:  9.0\n",
      "Epoch:  138 | Epoch_reward:  9.0\n",
      "Epoch:  139 | Epoch_reward:  11.0\n",
      "Epoch:  140 | Epoch_reward:  9.0\n",
      "Epoch:  141 | Epoch_reward:  20.0\n",
      "Epoch:  142 | Epoch_reward:  11.0\n",
      "Epoch:  143 | Epoch_reward:  11.0\n",
      "Epoch:  144 | Epoch_reward:  13.0\n",
      "Epoch:  145 | Epoch_reward:  11.0\n",
      "Epoch:  146 | Epoch_reward:  10.0\n",
      "Epoch:  147 | Epoch_reward:  9.0\n",
      "Epoch:  148 | Epoch_reward:  10.0\n",
      "Epoch:  149 | Epoch_reward:  11.0\n",
      "Epoch:  150 | Epoch_reward:  10.0\n",
      "Epoch:  151 | Epoch_reward:  12.0\n",
      "Epoch:  152 | Epoch_reward:  8.0\n",
      "Epoch:  153 | Epoch_reward:  9.0\n",
      "Epoch:  154 | Epoch_reward:  11.0\n",
      "Epoch:  155 | Epoch_reward:  10.0\n",
      "Epoch:  156 | Epoch_reward:  9.0\n",
      "Epoch:  157 | Epoch_reward:  12.0\n",
      "Epoch:  158 | Epoch_reward:  9.0\n",
      "Epoch:  159 | Epoch_reward:  10.0\n",
      "Epoch:  160 | Epoch_reward:  23.0\n",
      "Epoch:  161 | Epoch_reward:  10.0\n",
      "Epoch:  162 | Epoch_reward:  10.0\n",
      "Epoch:  163 | Epoch_reward:  9.0\n",
      "Epoch:  164 | Epoch_reward:  9.0\n",
      "Epoch:  165 | Epoch_reward:  10.0\n",
      "Epoch:  166 | Epoch_reward:  10.0\n",
      "Epoch:  167 | Epoch_reward:  12.0\n",
      "Epoch:  168 | Epoch_reward:  13.0\n",
      "Epoch:  169 | Epoch_reward:  10.0\n",
      "Epoch:  170 | Epoch_reward:  16.0\n",
      "Epoch:  171 | Epoch_reward:  10.0\n",
      "Epoch:  172 | Epoch_reward:  25.0\n",
      "Epoch:  173 | Epoch_reward:  9.0\n",
      "Epoch:  174 | Epoch_reward:  27.0\n",
      "Epoch:  175 | Epoch_reward:  9.0\n",
      "Epoch:  176 | Epoch_reward:  16.0\n",
      "Epoch:  177 | Epoch_reward:  9.0\n",
      "Epoch:  178 | Epoch_reward:  10.0\n",
      "Epoch:  179 | Epoch_reward:  23.0\n",
      "Epoch:  180 | Epoch_reward:  10.0\n",
      "Epoch:  181 | Epoch_reward:  10.0\n",
      "Epoch:  182 | Epoch_reward:  9.0\n",
      "Epoch:  183 | Epoch_reward:  8.0\n",
      "Epoch:  184 | Epoch_reward:  10.0\n",
      "Epoch:  185 | Epoch_reward:  8.0\n",
      "Epoch:  186 | Epoch_reward:  9.0\n",
      "Epoch:  187 | Epoch_reward:  13.0\n",
      "Epoch:  188 | Epoch_reward:  12.0\n",
      "Epoch:  189 | Epoch_reward:  10.0\n",
      "Epoch:  190 | Epoch_reward:  11.0\n",
      "Epoch:  191 | Epoch_reward:  9.0\n",
      "Epoch:  192 | Epoch_reward:  9.0\n",
      "Epoch:  193 | Epoch_reward:  10.0\n",
      "Epoch:  194 | Epoch_reward:  17.0\n",
      "Epoch:  195 | Epoch_reward:  13.0\n",
      "Epoch:  196 | Epoch_reward:  9.0\n",
      "Epoch:  197 | Epoch_reward:  9.0\n",
      "Epoch:  198 | Epoch_reward:  10.0\n",
      "Epoch:  199 | Epoch_reward:  9.0\n",
      "Epoch:  200 | Epoch_reward:  21.0\n",
      "Epoch:  201 | Epoch_reward:  9.0\n",
      "Epoch:  202 | Epoch_reward:  10.0\n",
      "Epoch:  203 | Epoch_reward:  10.0\n",
      "Epoch:  204 | Epoch_reward:  10.0\n",
      "Epoch:  205 | Epoch_reward:  24.0\n",
      "Epoch:  206 | Epoch_reward:  10.0\n",
      "Epoch:  207 | Epoch_reward:  14.0\n",
      "Epoch:  208 | Epoch_reward:  12.0\n",
      "Epoch:  209 | Epoch_reward:  9.0\n",
      "Epoch:  210 | Epoch_reward:  13.0\n",
      "Epoch:  211 | Epoch_reward:  10.0\n",
      "Epoch:  212 | Epoch_reward:  10.0\n",
      "Epoch:  213 | Epoch_reward:  12.0\n",
      "Epoch:  214 | Epoch_reward:  12.0\n",
      "Epoch:  215 | Epoch_reward:  10.0\n",
      "Epoch:  216 | Epoch_reward:  13.0\n",
      "Epoch:  217 | Epoch_reward:  9.0\n",
      "Epoch:  218 | Epoch_reward:  9.0\n",
      "Epoch:  219 | Epoch_reward:  10.0\n",
      "Epoch:  220 | Epoch_reward:  12.0\n",
      "Epoch:  221 | Epoch_reward:  11.0\n",
      "Epoch:  222 | Epoch_reward:  10.0\n",
      "Epoch:  223 | Epoch_reward:  27.0\n",
      "Epoch:  224 | Epoch_reward:  10.0\n",
      "Epoch:  225 | Epoch_reward:  9.0\n",
      "Epoch:  226 | Epoch_reward:  27.0\n",
      "Epoch:  227 | Epoch_reward:  10.0\n",
      "Epoch:  228 | Epoch_reward:  9.0\n",
      "Epoch:  229 | Epoch_reward:  9.0\n",
      "Epoch:  230 | Epoch_reward:  11.0\n",
      "Epoch:  231 | Epoch_reward:  10.0\n",
      "Epoch:  232 | Epoch_reward:  10.0\n",
      "Epoch:  233 | Epoch_reward:  23.0\n",
      "Epoch:  234 | Epoch_reward:  10.0\n",
      "Epoch:  235 | Epoch_reward:  11.0\n",
      "Epoch:  236 | Epoch_reward:  10.0\n",
      "Epoch:  237 | Epoch_reward:  9.0\n",
      "Epoch:  238 | Epoch_reward:  11.0\n",
      "Epoch:  239 | Epoch_reward:  15.0\n",
      "Epoch:  240 | Epoch_reward:  30.0\n",
      "Epoch:  241 | Epoch_reward:  9.0\n",
      "Epoch:  242 | Epoch_reward:  18.0\n",
      "Epoch:  243 | Epoch_reward:  8.0\n",
      "Epoch:  244 | Epoch_reward:  10.0\n",
      "Epoch:  245 | Epoch_reward:  27.0\n",
      "Epoch:  246 | Epoch_reward:  9.0\n",
      "Epoch:  247 | Epoch_reward:  12.0\n",
      "Epoch:  248 | Epoch_reward:  16.0\n",
      "Epoch:  249 | Epoch_reward:  9.0\n",
      "Epoch:  250 | Epoch_reward:  10.0\n",
      "Epoch:  251 | Epoch_reward:  13.0\n",
      "Epoch:  252 | Epoch_reward:  10.0\n",
      "Epoch:  253 | Epoch_reward:  11.0\n",
      "Epoch:  254 | Epoch_reward:  9.0\n",
      "Epoch:  255 | Epoch_reward:  10.0\n",
      "Epoch:  256 | Epoch_reward:  15.0\n",
      "Epoch:  257 | Epoch_reward:  10.0\n",
      "Epoch:  258 | Epoch_reward:  10.0\n",
      "Epoch:  259 | Epoch_reward:  18.0\n",
      "Epoch:  260 | Epoch_reward:  8.0\n",
      "Epoch:  261 | Epoch_reward:  11.0\n",
      "Epoch:  262 | Epoch_reward:  9.0\n",
      "Epoch:  263 | Epoch_reward:  10.0\n",
      "Epoch:  264 | Epoch_reward:  11.0\n",
      "Epoch:  265 | Epoch_reward:  44.0\n",
      "Epoch:  266 | Epoch_reward:  14.0\n",
      "Epoch:  267 | Epoch_reward:  11.0\n",
      "Epoch:  268 | Epoch_reward:  33.0\n",
      "Epoch:  269 | Epoch_reward:  13.0\n",
      "Epoch:  270 | Epoch_reward:  10.0\n",
      "Epoch:  271 | Epoch_reward:  15.0\n",
      "Epoch:  272 | Epoch_reward:  23.0\n",
      "Epoch:  273 | Epoch_reward:  9.0\n",
      "Epoch:  274 | Epoch_reward:  29.0\n",
      "Epoch:  275 | Epoch_reward:  13.0\n",
      "Epoch:  276 | Epoch_reward:  9.0\n",
      "Epoch:  277 | Epoch_reward:  9.0\n",
      "Epoch:  278 | Epoch_reward:  10.0\n",
      "Epoch:  279 | Epoch_reward:  14.0\n",
      "Epoch:  280 | Epoch_reward:  8.0\n",
      "Epoch:  281 | Epoch_reward:  18.0\n",
      "Epoch:  282 | Epoch_reward:  10.0\n",
      "Epoch:  283 | Epoch_reward:  10.0\n",
      "Epoch:  284 | Epoch_reward:  30.0\n",
      "Epoch:  285 | Epoch_reward:  8.0\n",
      "Epoch:  286 | Epoch_reward:  9.0\n",
      "Epoch:  287 | Epoch_reward:  11.0\n",
      "Epoch:  288 | Epoch_reward:  10.0\n",
      "Epoch:  289 | Epoch_reward:  14.0\n",
      "Epoch:  290 | Epoch_reward:  9.0\n",
      "Epoch:  291 | Epoch_reward:  17.0\n",
      "Epoch:  292 | Epoch_reward:  9.0\n",
      "Epoch:  293 | Epoch_reward:  8.0\n",
      "Epoch:  294 | Epoch_reward:  8.0\n",
      "Epoch:  295 | Epoch_reward:  9.0\n",
      "Epoch:  296 | Epoch_reward:  18.0\n",
      "Epoch:  297 | Epoch_reward:  8.0\n",
      "Epoch:  298 | Epoch_reward:  11.0\n",
      "Epoch:  299 | Epoch_reward:  22.0\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 300\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "    # Initialize the environment and get it's state\n",
    "    state=env.reset()\n",
    "    state = np.array([state], dtype=np.float32)\n",
    "    # 总奖励\n",
    "    ep_r = 0\n",
    "    while True:\n",
    "        action = select_action(state)\n",
    "        observation, reward, terminated, truncated = env.step(action.item())\n",
    "\n",
    "\n",
    "        ep_r += reward\n",
    "        reward = np.array([reward], np.float32)\n",
    "\n",
    "        done = terminated or truncated\n",
    "\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = np.array([observation], dtype=np.float32)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the eval network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.trainable_params()\n",
    "        policy_net_state_dict = eval_net.trainable_params()\n",
    "        for t_param, p_param in zip(target_net_state_dict, policy_net_state_dict):\n",
    "            ops.assign(t_param, (p_param*TAU + t_param*(1-TAU)))\n",
    "\n",
    "        if done:\n",
    "            print('Epoch: ', i_episode, '| Epoch_reward: ', round(ep_r, 2))\n",
    "\n",
    "            break\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4b605",
   "metadata": {},
   "source": [
    "完成数据处理、定义神经网络模型Net以及构建DQN之后，开始模型训练。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e4ab65fdc43de4932de46c69418c64f6e3769852db39afdc53804a8e5b8ff4a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
