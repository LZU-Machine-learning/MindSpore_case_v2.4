{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 基于MindSpore实现AlexNet手写字体识别\n",
    "\n",
    "本实验主要介绍使用华为的深度学习框架MindSpore实现的AlexNet模型，用于手写字体的图像分类任务，使用的数据集为MNIST手写数字数据集。该实验旨在展示如何使用MindSpore框架构建深度学习模型，并在手写数字分类任务中进行训练和测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.实验目的\n",
    "\n",
    "- 掌握如何使用MindSpore框架构建深度学习模型。\n",
    "\n",
    "- 掌握如何在手写数字分类任务中使用MindSpore进行数据处理、模型构建、训练和评估。\n",
    "\n",
    "- 了解AlexNet模型的基本结构和工作原理。\n",
    "\n",
    "- 通过手写数字识别任务掌握深度学习在计算机视觉中的应用。\n",
    "\n",
    "- 掌握如何使用深度学习模型解决实际问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.AlexNet模型原理介绍\n",
    "\n",
    "[AlexNet](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)是一种深度卷积神经网络模型，是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton在2012年ImageNet大规模视觉识别竞赛（ImageNet Large Scale Visual Recognition Challenge, ILSVRC）中提出并夺得冠军的。AlexNet模型的主要结构包括5个卷积层和3个全连接层，其中，前5个卷积层之间以及最后2个全连接层之间使用了ReLU激活函数，而最后一个全连接层使用了softmax激活函数。\n",
    "\n",
    "![jupyter](./Figures/Fig002.png)\n",
    "\n",
    "[AlexNet](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)的卷积层采用了较大的卷积核（11x11、5x5），较大的步幅（4、2），以及较多的卷积通道（96、256），这样的设计使得模型具有更强的表征能力和更高的分类准确度。同时，在模型训练时，[AlexNet](http://www.cs.toronto.edu/~hinton/absps/imagenet.pdf)采用了数据增强和dropout等技术，以避免模型出现过拟合现象，提高模型的泛化能力。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.实验环境\n",
    "\n",
    "在动手进行实践之前，需要注意以下几点：\n",
    "* 确保实验环境正确安装，包括安装MindSpore。安装过程：首先登录[MindSpore官网安装页面](https://www.mindspore.cn/install)，根据安装指南下载安装包及查询相关文档。同时，官网环境安装也可以按下表说明找到对应环境搭建文档链接，根据环境搭建手册配置对应的实验环境。\n",
    "* 推荐使用交互式的计算环境Jupyter Notebook，其交互性强，易于可视化，适合频繁修改的数据分析实验环境。\n",
    "* 实验也可以在华为云一站式的AI开发平台ModelArts上完成。\n",
    "* 推荐实验环境：MindSpore版本=MindSpore 2.4；Python环境=3.11。\n",
    "\n",
    "\n",
    "|  硬件平台 |  操作系统  |          软件环境           | 开发环境 | 环境搭建链接 |\n",
    "| :-----:| :----: |:-----------------------:|:----:   |:----:   |\n",
    "| CPU | Windows-x64 | MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.1节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| GPU |Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第二章2.2节和第三章3.1节](./MindSpore环境搭建实验手册.docx)|\n",
    "| Ascend 910  | Linux-x86_64| MindSpore2.4 Python3.11 | JupyterNotebook |[MindSpore环境搭建实验手册第四章](./MindSpore环境搭建实验手册.docx)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.数据处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1数据准备\n",
    "\n",
    "MNIST数据集是机器学习中常用的数据集之一，它包含了70,000张手写数字图片，每张图片都是28x28像素大小的灰度图像，数字从0到9。该数据集由美国国家标准与技术研究所（NIST）于1999年创建，而后由Yann LeCun等人进行改进和维护，因此被称为MNIST（Modified NIST）数据集。\n",
    "\n",
    "MNIST数据集主要用于图像识别领域的研究，特别是用于机器学习算法的测试和比较。由于它的简单性和易于使用，MNIST已经成为机器学习社区中的标准数据集之一。\n",
    "\n",
    "以下是几个示例图像：\n",
    "\n",
    "![jupyter](./Figures/Fig001.png)\n",
    "\n",
    "每个图像都被转换成784个数字的一维向量（28×28=784）。这些数字表示像素的灰度值，值的范围从0（黑色）到255（白色）。为了将这些向量转换成有意义的输出，通常使用分类算法，例如支持向量机（SVM）或神经网络。分类算法的目标是将每个图像正确地标记为它所代表的数字。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2数据加载\n",
    "\n",
    "MindSpore提供了MNIST数据集并将其划分为训练集和测试集，我们只需下载载入即可。\n",
    "我们可以通过MnistDataset函数载入下载好的数据集，并且get_col_names()提供了查看数据集中所有列名称的功能。\n",
    "载入完成数据后，我们需要对数据集进行预处理，我们通过datapipe()函数实现对图片的预处理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:38:49.793261Z",
     "start_time": "2025-01-13T12:38:47.615874Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/notebook/datasets/MNIST_Data.zip (10.3 MB)\n",
      "\n",
      "file_sizes: 100%|██████████████████████████| 10.8M/10.8M [00:00<00:00, 20.3MB/s]\n",
      "Extracting zip file...\n",
      "Successfully downloaded / unzipped to ./\n"
     ]
    }
   ],
   "source": [
    "import mindspore \n",
    "from mindspore import nn as nn\n",
    "from mindspore.dataset import vision, transforms\n",
    "from mindspore.dataset import MnistDataset\n",
    "\n",
    "\n",
    "#下载数据集\n",
    "from download import download    \n",
    "\n",
    "url = \"https://mindspore-website.obs.cn-north-4.myhuaweicloud.com/\" \\\n",
    "      \"notebook/datasets/MNIST_Data.zip\"\n",
    "path = download(url, \"./\", kind=\"zip\", replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:38:52.160336Z",
     "start_time": "2025-01-13T12:38:52.153352Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['image', 'label']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 载入数据集\n",
    "train_dataset = MnistDataset('MNIST_Data/train')\n",
    "test_dataset = MnistDataset('MNIST_Data/test')\n",
    "\n",
    "# 获取数据集中所有列的名称\n",
    "train_dataset.get_col_names() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:38:55.598293Z",
     "start_time": "2025-01-13T12:38:55.593441Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# datapipe完成映射图像变换和将大规模数据集分割成多个小批次的数据集\n",
    "def datapipe(dataset, batch_size):\n",
    "    image_transforms = [\n",
    "        vision.Rescale(1.0 / 255.0, 0),\n",
    "        vision.Resize((32, 32)),\n",
    "        vision.HWC2CHW()\n",
    "        \n",
    "    ]\n",
    "    label_transform = transforms.TypeCast(mindspore.int32)\n",
    "\n",
    "    dataset = dataset.map(image_transforms, 'image')\n",
    "    dataset = dataset.map(label_transform, 'label')\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 数据集预处理\n",
    "train_dataset = datapipe(train_dataset, 16)\n",
    "test_dataset = datapipe(test_dataset, 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:38:58.259238Z",
     "start_time": "2025-01-13T12:38:58.207744Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of image [N, C, H, W]: (16, 1, 32, 32) Float32\n",
      "Shape of label: (16,) Int32\n",
      "Shape of image [N, C, H, W]: (16, 1, 32, 32) Float32\n",
      "Shape of label: (16,) Int32\n"
     ]
    }
   ],
   "source": [
    "# 结果展示 每个image包含16张图片，每张图片是单通道，高32像素，宽32像素\n",
    "for image, label in test_dataset.create_tuple_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {image.shape} {image.dtype}\")\n",
    "    print(f\"Shape of label: {label.shape} {label.dtype}\")\n",
    "    break\n",
    "\n",
    "for data in test_dataset.create_dict_iterator():\n",
    "    print(f\"Shape of image [N, C, H, W]: {data['image'].shape} {data['image'].dtype}\")\n",
    "    print(f\"Shape of label: {data['label'].shape} {data['label'].dtype}\")\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.模型构建\n",
    "\n",
    "构建一个减少卷积层的精简版AlexNet：第一层为conv1，采用relu激活函数，第二层为最大池化层，第三层为conv2，采用relu激活函数，第四层为最大池化层，第五层为全连接层，第六层为全连接层，第七层为全连接层。使用交叉熵损失函数，和Adam优化器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:39:02.044565Z",
     "start_time": "2025-01-13T12:39:02.031749Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "AlexNet的原始输入图片大小为224*224\n",
    "Mnist数据集中图片大小为28*28\n",
    "所以需要对网络进行精简，减少两层卷积层。\n",
    "'''\n",
    "\n",
    "class AlexNet(nn.Cell):\n",
    "    def __init__(self, num_class=10, num_channel=1):\n",
    "        super(AlexNet, self).__init__()\n",
    "        # 卷积层，输入的通道数为num_channel，输出的通道数为6，卷积核大小为5*5\n",
    "        self.conv1 = nn.Conv2d(num_channel, 6, 5, pad_mode='valid')\n",
    "        # 卷积层，输入的通道数为6，输出的通道数为16，卷积核大小为5*5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5, pad_mode='valid')\n",
    "        # 全连接层，输入个数为16*5*5，输出个数为120\n",
    "        self.fc1 = nn.Dense(16 * 5 * 5, 120)\n",
    "        # 全连接层，输入个数为120，输出个数为84\n",
    "        self.fc2 = nn.Dense(120, 84)\n",
    "        # 全连接层，输入个数为84，分类的个数为num_class\n",
    "        self.fc3 = nn.Dense(84, num_class)\n",
    "        # ReLU激活函数\n",
    "        self.relu = nn.ReLU()\n",
    "        # 池化层\n",
    "        self.max_pool2d = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # 多维数组展平为一维数组\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def construct(self, x):\n",
    "        # 使用定义好的运算构建前向网络\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2d(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = AlexNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:39:04.210555Z",
     "start_time": "2025-01-13T12:39:04.199422Z"
    }
   },
   "outputs": [],
   "source": [
    "# 损失函数、优化器\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = nn.Adam(params=model.trainable_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.训练模型\n",
    "\n",
    "此部分分为两个部分，训练和测试。测试部分不需要计算梯度通过model.set_train(False)实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:39:08.054209Z",
     "start_time": "2025-01-13T12:39:08.048454Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, dataset, loss_fn, optimizer):\n",
    "    # 定义前向传播\n",
    "    def forward_fn(data, label):\n",
    "        logits = model(data)\n",
    "        loss = loss_fn(logits, label)\n",
    "        return loss, logits\n",
    "\n",
    "    # 获取梯度\n",
    "    grad_fn = mindspore.value_and_grad(forward_fn, None, optimizer.parameters, has_aux=True)\n",
    "\n",
    "    # 获取每个batch损失\n",
    "    def train_step(data, label):\n",
    "        (loss, _), grads = grad_fn(data, label)\n",
    "        optimizer(grads)\n",
    "        return loss\n",
    "\n",
    "    size = dataset.get_dataset_size()\n",
    "    model.set_train()\n",
    "    for batch, (data, label) in enumerate(dataset.create_tuple_iterator()):\n",
    "        loss = train_step(data, label)\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.asnumpy(), batch\n",
    "            print(f\"loss: {loss:>7f}  [{current:>3d}/{size:>3d}]\")\n",
    "\n",
    "\n",
    "def test(model, dataset, loss_fn):\n",
    "    num_batches = dataset.get_dataset_size()\n",
    "    model.set_train(False)\n",
    "    total, test_loss, correct = 0, 0, 0\n",
    "    for data, label in dataset.create_tuple_iterator():\n",
    "        pred = model(data)\n",
    "        total += len(data)\n",
    "        test_loss += loss_fn(pred, label).asnumpy()\n",
    "        correct += (pred.argmax(1) == label).asnumpy().sum()\n",
    "    test_loss /= num_batches\n",
    "    correct /= total\n",
    "    print(f\"Test: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:42:05.800021Z",
     "start_time": "2025-01-13T12:39:22.480654Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.282120  [  0/3750]\n",
      "loss: 0.658918  [100/3750]\n",
      "loss: 0.405409  [200/3750]\n",
      "loss: 0.067219  [300/3750]\n",
      "loss: 0.325809  [400/3750]\n",
      "loss: 0.047502  [500/3750]\n",
      "loss: 0.302238  [600/3750]\n",
      "loss: 0.240312  [700/3750]\n",
      "loss: 0.058779  [800/3750]\n",
      "loss: 0.291428  [900/3750]\n",
      "loss: 0.096431  [1000/3750]\n",
      "loss: 0.006517  [1100/3750]\n",
      "loss: 0.133028  [1200/3750]\n",
      "loss: 0.007033  [1300/3750]\n",
      "loss: 0.241615  [1400/3750]\n",
      "loss: 0.056639  [1500/3750]\n",
      "loss: 0.485809  [1600/3750]\n",
      "loss: 0.045098  [1700/3750]\n",
      "loss: 0.054939  [1800/3750]\n",
      "loss: 0.018392  [1900/3750]\n",
      "loss: 0.077691  [2000/3750]\n",
      "loss: 0.158039  [2100/3750]\n",
      "loss: 0.101539  [2200/3750]\n",
      "loss: 0.017793  [2300/3750]\n",
      "loss: 0.249777  [2400/3750]\n",
      "loss: 0.033514  [2500/3750]\n",
      "loss: 0.013644  [2600/3750]\n",
      "loss: 0.162629  [2700/3750]\n",
      "loss: 0.179173  [2800/3750]\n",
      "loss: 0.219760  [2900/3750]\n",
      "loss: 0.015519  [3000/3750]\n",
      "loss: 0.146912  [3100/3750]\n",
      "loss: 0.054841  [3200/3750]\n",
      "loss: 0.013830  [3300/3750]\n",
      "loss: 0.013988  [3400/3750]\n",
      "loss: 0.009856  [3500/3750]\n",
      "loss: 0.412076  [3600/3750]\n",
      "loss: 0.005157  [3700/3750]\n",
      "Test: \n",
      " Accuracy: 98.1%, Avg loss: 0.052663 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.027708  [  0/3750]\n",
      "loss: 0.014982  [100/3750]\n",
      "loss: 0.013819  [200/3750]\n",
      "loss: 0.049290  [300/3750]\n",
      "loss: 0.090480  [400/3750]\n",
      "loss: 0.004651  [500/3750]\n",
      "loss: 0.011071  [600/3750]\n",
      "loss: 0.102383  [700/3750]\n",
      "loss: 0.000587  [800/3750]\n",
      "loss: 0.015538  [900/3750]\n",
      "loss: 0.005952  [1000/3750]\n",
      "loss: 0.158977  [1100/3750]\n",
      "loss: 0.070470  [1200/3750]\n",
      "loss: 0.015777  [1300/3750]\n",
      "loss: 0.002991  [1400/3750]\n",
      "loss: 0.000485  [1500/3750]\n",
      "loss: 0.021207  [1600/3750]\n",
      "loss: 0.000546  [1700/3750]\n",
      "loss: 0.012147  [1800/3750]\n",
      "loss: 0.014319  [1900/3750]\n",
      "loss: 0.012443  [2000/3750]\n",
      "loss: 0.016163  [2100/3750]\n",
      "loss: 0.008435  [2200/3750]\n",
      "loss: 0.023807  [2300/3750]\n",
      "loss: 0.004165  [2400/3750]\n",
      "loss: 0.007075  [2500/3750]\n",
      "loss: 0.152326  [2600/3750]\n",
      "loss: 0.124126  [2700/3750]\n",
      "loss: 0.000255  [2800/3750]\n",
      "loss: 0.082763  [2900/3750]\n",
      "loss: 0.028226  [3000/3750]\n",
      "loss: 0.004061  [3100/3750]\n",
      "loss: 0.013563  [3200/3750]\n",
      "loss: 0.075940  [3300/3750]\n",
      "loss: 0.033772  [3400/3750]\n",
      "loss: 0.081880  [3500/3750]\n",
      "loss: 0.002673  [3600/3750]\n",
      "loss: 0.030580  [3700/3750]\n",
      "Test: \n",
      " Accuracy: 98.2%, Avg loss: 0.054536 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.002257  [  0/3750]\n",
      "loss: 0.011025  [100/3750]\n",
      "loss: 0.049553  [200/3750]\n",
      "loss: 0.001045  [300/3750]\n",
      "loss: 0.000651  [400/3750]\n",
      "loss: 0.005881  [500/3750]\n",
      "loss: 0.002627  [600/3750]\n",
      "loss: 0.003145  [700/3750]\n",
      "loss: 0.067254  [800/3750]\n",
      "loss: 0.004932  [900/3750]\n",
      "loss: 0.000437  [1000/3750]\n",
      "loss: 0.013299  [1100/3750]\n",
      "loss: 0.622346  [1200/3750]\n",
      "loss: 0.331099  [1300/3750]\n",
      "loss: 0.005735  [1400/3750]\n",
      "loss: 0.040129  [1500/3750]\n",
      "loss: 0.195603  [1600/3750]\n",
      "loss: 0.003592  [1700/3750]\n",
      "loss: 0.000723  [1800/3750]\n",
      "loss: 0.003977  [1900/3750]\n",
      "loss: 0.158399  [2000/3750]\n",
      "loss: 0.138130  [2100/3750]\n",
      "loss: 0.003743  [2200/3750]\n",
      "loss: 0.007747  [2300/3750]\n",
      "loss: 0.098463  [2400/3750]\n",
      "loss: 0.010050  [2500/3750]\n",
      "loss: 0.376003  [2600/3750]\n",
      "loss: 0.060844  [2700/3750]\n",
      "loss: 0.087170  [2800/3750]\n",
      "loss: 0.002973  [2900/3750]\n",
      "loss: 0.054966  [3000/3750]\n",
      "loss: 0.001472  [3100/3750]\n",
      "loss: 0.001099  [3200/3750]\n",
      "loss: 0.003820  [3300/3750]\n",
      "loss: 0.001358  [3400/3750]\n",
      "loss: 0.016794  [3500/3750]\n",
      "loss: 0.002986  [3600/3750]\n",
      "loss: 0.001993  [3700/3750]\n",
      "Test: \n",
      " Accuracy: 98.8%, Avg loss: 0.038038 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#训练3个周期\n",
    "epochs = 3\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(model, train_dataset, loss_fn, optimizer)\n",
    "    #每个周期训练完的模型在测试集上验证\n",
    "    test(model, test_dataset, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.模型预测\n",
    "\n",
    "MindSpore提供save_checkpoint()函数保存模型和load_checkpoint()函数载入模型，我们载入刚才训练好的模型，对测试集再次进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:46:18.364340Z",
     "start_time": "2025-01-13T12:46:18.355791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Model to model.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 模型保存\n",
    "mindspore.save_checkpoint(model, \"model.ckpt\")\n",
    "print(\"Saved Model to model.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:46:20.550105Z",
     "start_time": "2025-01-13T12:46:20.527093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([], [])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 实例化模型\n",
    "model = AlexNet()\n",
    "# 载入训练好的模型\n",
    "param_dict = mindspore.load_checkpoint(\"model.ckpt\")\n",
    "param_not_load = mindspore.load_param_into_net(model, param_dict)\n",
    "print(param_not_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T12:46:22.737504Z",
     "start_time": "2025-01-13T12:46:22.698653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"[5 6 9 7 8 1 2 4 8 9]\", Actual: \"[5 6 9 7 8 1 2 4 2 9]\"\n"
     ]
    }
   ],
   "source": [
    "model.set_train(False)\n",
    "for data, label in test_dataset:\n",
    "    pred = model(data)\n",
    "    predicted = pred.argmax(1)\n",
    "    print(f'Predicted: \"{predicted[:10]}\", Actual: \"{label[:10]}\"')\n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "26fcd935751ef84cad2d7f9e4bd00a41b458c58fd62c8d14685a9368156264ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
